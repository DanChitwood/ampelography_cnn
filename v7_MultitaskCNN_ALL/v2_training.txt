# V2 TRAINING

import os
import numpy as np
import random
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, r2_score # ADDED r2_score
from scipy.stats import pearsonr # ADDED pearsonr
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.preprocessing import LabelEncoder
import joblib # For saving/loading scaler

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# ====================
# Device setup
# ====================
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ====================
# 1. Preprocessing Function (NO CHANGE HERE)
# ====================
def preprocess_leaf_masks(data_folder="data",
                          PCs=None,
                          names=None,
                          labels=None,
                          resolution=512,
                          n_rotations=20,
                          output_folder="PIXEL_IMAGES",
                          normalize_pcs=True):

    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(os.path.join(output_folder, "MASKS"), exist_ok=True)
    os.makedirs(os.path.join(output_folder, "RGB_IMAGES"), exist_ok=True)

    assert PCs is not None and names is not None and labels is not None
    assert len(names) == len(PCs) == len(labels)

    all_filenames_raw = []
    all_labels_raw = []
    all_pcs_raw = []

    counter = 1

    for i, name in enumerate(names):
        label = labels[i]
        pc = PCs[i]

        blade_candidates = [f for f in os.listdir(data_folder) if name in f and "blade" in f]
        if not blade_candidates:
            print(f"ERROR: No blade file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        blade_file = blade_candidates[0]

        vein_candidates = [f for f in os.listdir(data_folder) if name in f and "veins" in f]
        if not vein_candidates:
            print(f"ERROR: No vein file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        vein_file = vein_candidates[0]

        blade = np.loadtxt(os.path.join(data_folder, blade_file))
        vein = np.loadtxt(os.path.join(data_folder, vein_file))
        all_coords = np.vstack([blade, vein])

        for rot in range(n_rotations):
            angle_deg = random.uniform(0, 360)
            angle_rad = np.deg2rad(angle_deg)
            R = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],
                          [np.sin(angle_rad),  np.cos(angle_rad)]])

            centroid = all_coords.mean(axis=0)
            rotated = (all_coords - centroid) @ R.T

            min_xy = rotated.min(axis=0)
            max_xy = rotated.max(axis=0)
            scale = resolution * 0.9 / max(max_xy - min_xy)
            scaled = (rotated - min_xy) * scale
            offset = (resolution - (max_xy - min_xy) * scale) / 2
            final_coords = scaled + offset

            n_blade = len(blade)
            blade_scaled = final_coords[:n_blade]
            vein_scaled = final_coords[n_blade:]

            mask = Image.new("L", (resolution, resolution), 0)
            draw = ImageDraw.Draw(mask)
            draw.polygon(blade_scaled.flatten().tolist(), fill=1)
            draw.polygon(vein_scaled.flatten().tolist(), fill=2)
            
            mask_name = f"{label}_{counter:04d}.png"
            mask.save(os.path.join(output_folder, "MASKS", mask_name))

            rgb = Image.new("RGB", (resolution, resolution), (0, 0, 0))
            rgb_draw = ImageDraw.Draw(rgb)
            rgb_draw.polygon(blade_scaled.flatten().tolist(), fill=(255, 140, 0))
            rgb_draw.polygon(vein_scaled.flatten().tolist(), fill=(255, 0, 255))
            rgb.save(os.path.join(output_folder, "RGB_IMAGES", mask_name))

            all_pcs_raw.append(pc)
            all_labels_raw.append(label)
            all_filenames_raw.append(mask_name)

            counter += 1

    metadata_df = pd.DataFrame({
        "filename": all_filenames_raw,
        "label": all_labels_raw,
        "PC1": [p[0] for p in all_pcs_raw],
        "PC2": [p[1] for p in all_pcs_raw]
    })

    if normalize_pcs:
        print("Normalizing PC values...")
        scaler = StandardScaler()
        metadata_df[['PC1', 'PC2']] = scaler.fit_transform(metadata_df[['PC1', 'PC2']])
        joblib.dump(scaler, os.path.join(output_folder, "pc_scaler.pkl"))
        print("PC values normalized and scaler saved.")

    metadata_df.to_csv(os.path.join(output_folder, "METADATA.csv"), index=False)
    
    print(f"Saved {len(metadata_df)} rotated leaf masks and metadata to '{output_folder}'.")


# ====================
# 2. Dataset class (NO CHANGE HERE)
# ====================
class LeafDataset(Dataset):
    def __init__(self, image_dir, metadata_file, transform=None):
        self.image_dir = image_dir
        self.metadata_df = pd.read_csv(metadata_file)
        self.transform = transform

        self.filenames = self.metadata_df['filename'].values
        self.label_encoder = LabelEncoder()
        self.labels = self.label_encoder.fit_transform(self.metadata_df['label'])
        self.pcs = self.metadata_df[["PC1", "PC2"]].values.astype(np.float32)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.filenames[idx])
        image = Image.open(img_path).convert("L")
        
        if self.transform:
            image = self.transform(image)

        label = torch.tensor(self.labels[idx], dtype=torch.long)
        pcs = torch.tensor(self.pcs[idx], dtype=torch.float32)
        
        return image, label, pcs, self.filenames[idx]


# ====================
# 3. CNN model (NO CHANGE HERE - REGRESSOR HEAD IS ALREADY DEEPENED)
# ====================
class CNNMultitask(nn.Module):
    def __init__(self, n_classes, resolution=512):
        super(CNNMultitask, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.flatten = nn.Flatten()
        
        self.flattened_size = 128 * (resolution // 16) * (resolution // 16)
        
        self.fc_embedding = nn.Linear(self.flattened_size, 256)
        self.dropout_embedding = nn.Dropout(0.3)

        # Regressor Head (now 4 layers deep to capture more complexity)
        self.regressor_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )
        
        # Classifier Head (kept as one linear layer for simplicity as it worked well)
        self.classifier_head = nn.Sequential(
            nn.Linear(256, n_classes) 
        )

    def forward(self, x):
        x = self.features(x)
        x = self.flatten(x)
        
        if x.shape[1] != self.flattened_size:
            raise ValueError(f"Flattened feature size mismatch. Expected {self.flattened_size}, got {x.shape[1]}. "
                             "Please ensure your input image resolution is compatible with the CNN layers, "
                             "e.g., 512 for this model, or adjust CNN layers/use AdaptiveAvgPool2d.")

        embeddings = self.dropout_embedding(F.relu(self.fc_embedding(x)))
        
        pred_pcs = self.regressor_head(embeddings)
        pred_labels = self.classifier_head(embeddings)
        
        return pred_pcs, pred_labels, embeddings


# ====================
# 4. Training Function (MODIFIED ALPHA)
# ====================
def train_model(data_dir="PIXEL_IMAGES",
                batch_size=16,
                lr=3e-4, 
                epochs=150, 
                alpha=0.25, # CHANGED: Lowered alpha from 0.3 to 0.25
                eval_interval=5):

    metadata_file = os.path.join(data_dir, "METADATA.csv")
    image_dir = os.path.join(data_dir, "MASKS")

    transform = transforms.Compose([
        transforms.ToTensor(),
    ])

    full_dataset = LeafDataset(image_dir, metadata_file, transform)
    
    original_labels = full_dataset.metadata_df['label'].values
    label_encoder_for_weights = LabelEncoder()
    encoded_labels = label_encoder_for_weights.fit_transform(original_labels)

    n_classes = len(np.unique(encoded_labels))

    class_counts = pd.Series(encoded_labels).value_counts().sort_index()
    total_samples = len(encoded_labels)
    class_weights = total_samples / (n_classes * class_counts.values) 
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    print(f"Calculated class weights (Inverse Frequency): {class_weights}")


    train_indices, val_indices = train_test_split(
        range(len(full_dataset)), test_size=0.2, random_state=42, 
        stratify=full_dataset.labels
    )
    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_indices), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_indices), batch_size=batch_size)

    model = CNNMultitask(n_classes).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion_reg = nn.MSELoss()
    criterion_cls = nn.CrossEntropyLoss(weight=class_weights)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)

    print(f"\nStarting training for {epochs} epochs...")
    print(f"  Training samples: {len(train_indices)}")
    print(f"  Validation samples: {len(val_indices)}")
    print(f"  Number of classes: {n_classes}")
    print(f"  Initial Learning Rate: {lr}")
    print(f"  Alpha (Classification Loss Weight): {alpha}")


    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        total_reg_loss = 0
        total_cls_loss = 0

        for batch_idx, (imgs, labels, pcs, filenames) in enumerate(train_loader):
            imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)

            optimizer.zero_grad()
            
            pred_pcs, pred_labels, _ = model(imgs)
            
            loss_reg = criterion_reg(pred_pcs, pcs)
            loss_cls = criterion_cls(pred_labels, labels)
            
            loss = loss_reg + alpha * loss_cls

            loss.backward()
            optimizer.step()
            
            total_train_loss += loss.item()
            total_reg_loss += loss_reg.item()
            total_cls_loss += loss_cls.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} (Reg: {total_reg_loss/len(train_loader):.4f}, Cls: {total_cls_loss/len(train_loader):.4f})")

        if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:
            model.eval()
            total_val_loss = 0
            with torch.no_grad():
                for imgs, labels, pcs, filenames in val_loader:
                    imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)
                    pred_pcs, pred_labels, _ = model(imgs)
                    loss = criterion_reg(pred_pcs, pcs) + alpha * criterion_cls(pred_labels, labels)
                    total_val_loss += loss.item()
            
            avg_val_loss = total_val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            print(f"  Validation Loss: {avg_val_loss:.4f}")
            
            scheduler.step(avg_val_loss)
            
    model_save_path = os.path.join(data_dir, "cnn_multitask_model.pt")
    torch.save(model.state_dict(), model_save_path)
    print(f"\n✅ Model saved to {model_save_path}")

    plt.figure(figsize=(10, 6))
    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')
    val_epochs = [e for e in range(1, epochs + 1) if (e % eval_interval == 0 or e == epochs)]
    plt.plot(val_epochs, val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(data_dir, "training_validation_loss.png"))
    plt.close()

    return model, train_losses, val_losses


# ====================
# 5. Evaluation Function (MODIFIED FOR DATA SPLIT & NEW METRICS)
# ====================
def evaluate_model(data_dir="PIXEL_IMAGES", data_split='all'): # ADDED data_split argument
    image_dir = os.path.join(data_dir, "MASKS")
    metadata_csv = os.path.join(data_dir, "METADATA.csv")
    model_path = os.path.join(data_dir, "cnn_multitask_model.pt")
    output_dir = os.path.join(data_dir, f"CNN_EVAL_OUTPUTS_{data_split.upper()}") # Changed output folder name
    os.makedirs(output_dir, exist_ok=True)

    transform = transforms.Compose([transforms.ToTensor()])
    
    full_dataset = LeafDataset(image_dir, metadata_csv, transform)

    if data_split == 'validation': # Logic to get validation data only
        _, val_indices = train_test_split(
            range(len(full_dataset)), test_size=0.2, random_state=42, 
            stratify=full_dataset.labels
        )
        dataset_to_evaluate = torch.utils.data.Subset(full_dataset, val_indices)
        eval_title_suffix = " (Validation Data)"
    else: # Default to 'all' data
        dataset_to_evaluate = full_dataset
        eval_title_suffix = ""

    dataloader = DataLoader(dataset_to_evaluate, batch_size=16, shuffle=False)

    model = CNNMultitask(n_classes=len(np.unique(full_dataset.labels))).to(device) # Use full_dataset.labels to get all classes
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    scaler = None
    scaler_path = os.path.join(data_dir, "pc_scaler.pkl")
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print("Loaded PC scaler for inverse transformation.")

    all_preds_pcs = []
    all_true_pcs = []
    all_preds_labels = []
    all_true_labels = []
    all_embeddings = []
    all_filenames = []

    with torch.no_grad():
        for images, labels, pcs, filenames in dataloader:
            images = images.to(device)
            pcs = pcs.to(device)
            labels = labels.to(device)

            preds_pcs, preds_labels, embeddings = model(images)

            all_preds_pcs.append(preds_pcs.cpu().numpy())
            all_true_pcs.append(pcs.cpu().numpy())
            all_preds_labels.append(preds_labels.argmax(dim=1).cpu().numpy())
            all_true_labels.append(labels.cpu().numpy())
            all_embeddings.append(embeddings.cpu().numpy())
            all_filenames.extend(filenames)

    pred_pcs_norm = np.vstack(all_preds_pcs)
    true_pcs_norm = np.vstack(all_true_pcs)
    pred_labels = np.concatenate(all_preds_labels)
    true_labels = np.concatenate(all_true_labels)
    embeddings = np.vstack(all_embeddings)
    
    # Use the label_encoder from the full_dataset to ensure consistent mapping
    label_names = full_dataset.label_encoder.inverse_transform(np.unique(full_dataset.labels)) 


    if scaler is not None:
        pred_pcs = scaler.inverse_transform(pred_pcs_norm)
        true_pcs = scaler.inverse_transform(true_pcs_norm)
        print("Inverse transformed PCs for plotting.")
    else:
        pred_pcs = pred_pcs_norm
        true_pcs = true_pcs_norm

    # ----- Regression Metrics (R2 and Pearsonr) ----- # ADDED
    r2_pc1 = r2_score(true_pcs[:, 0], pred_pcs[:, 0])
    r2_pc2 = r2_score(true_pcs[:, 1], pred_pcs[:, 1])
    pearson_pc1, _ = pearsonr(true_pcs[:, 0], pred_pcs[:, 0])
    pearson_pc2, _ = pearsonr(true_pcs[:, 1], pred_pcs[:, 1])

    print(f"\n--- Regression Metrics {eval_title_suffix} ---")
    print(f"R2 Score (PC1): {r2_pc1:.4f}")
    print(f"Pearson Correlation (PC1): {pearson_pc1:.4f}")
    print(f"R2 Score (PC2): {r2_pc2:.4f}")
    print(f"Pearson Correlation (PC2): {pearson_pc2:.4f}")


    # ----- Save PC scatter (PC1 vs PC2) -----
    plt.figure(figsize=(8, 8))
    plt.scatter(true_pcs[:, 0], true_pcs[:, 1], label="True PCs", alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
    plt.scatter(pred_pcs[:, 0], pred_pcs[:, 1], label="Predicted PCs", alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title(f"Predicted vs Ground Truth PC1/PC2{eval_title_suffix}") # MODIFIED TITLE
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"pc_scatter_PC1_vs_PC2{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
    plt.close()

    # ----- Save PC1 Predicted vs True -----
    plt.figure(figsize=(6, 6))
    plt.scatter(true_pcs[:, 0], pred_pcs[:, 0], alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
    min_pc1, max_pc1 = np.min(true_pcs[:, 0]), np.max(true_pcs[:, 0])
    plt.plot([min_pc1, max_pc1], [min_pc1, max_pc1], 'r--', label='Ideal (y=x)')
    plt.xlabel("True PC1")
    plt.ylabel("Predicted PC1")
    plt.title(f"Predicted vs True PC1{eval_title_suffix}") # MODIFIED TITLE
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"pc_scatter_PC1_pred_vs_true{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
    plt.close()

    # ----- Save PC2 Predicted vs True -----
    plt.figure(figsize=(6, 6))
    plt.scatter(true_pcs[:, 1], pred_pcs[:, 1], alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
    min_pc2, max_pc2 = np.min(true_pcs[:, 1]), np.max(true_pcs[:, 1])
    plt.plot([min_pc2, max_pc2], [min_pc2, max_pc2], 'r--', label='Ideal (y=x)')
    plt.xlabel("True PC2")
    plt.ylabel("Predicted PC2")
    plt.title(f"Predicted vs True PC2{eval_title_suffix}") # MODIFIED TITLE
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"pc_scatter_PC2_pred_vs_true{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
    plt.close()


    # ----- Confusion Matrix -----
    cm = confusion_matrix(true_labels, pred_labels)
    # Ensure display_labels are consistent with the full set of classes, even if not all appear in a subset
    disp_labels_for_cm = full_dataset.label_encoder.inverse_transform(full_dataset.label_encoder.transform(full_dataset.metadata_df['label'].unique()))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels_for_cm)
    fig, ax = plt.subplots(figsize=(10, 10))
    disp.plot(ax=ax, xticks_rotation=90)
    plt.title(f"Classification Confusion Matrix{eval_title_suffix}") # MODIFIED TITLE
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"confusion_matrix{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
    plt.close()

    # ----- t-SNE -----
    perplexity_val = min(30, len(embeddings) - 1)
    if perplexity_val > 1:
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)
        emb_tsne = tsne.fit_transform(embeddings)
        plt.figure(figsize=(8, 8))
        # Ensure that label_names corresponds to the labels present in the current true_labels
        current_label_names = full_dataset.label_encoder.inverse_transform(np.unique(true_labels))
        for i, name in enumerate(current_label_names):
            # Map back to original encoded labels for indexing
            original_encoded_label = full_dataset.label_encoder.transform([name])[0]
            idxs = true_labels == original_encoded_label
            plt.scatter(emb_tsne[idxs, 0], emb_tsne[idxs, 1], label=name, alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
        plt.legend(fontsize="x-small", loc="best")
        plt.title(f"t-SNE of CNN Embeddings{eval_title_suffix}") # MODIFIED TITLE
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"tsne_embeddings{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
        plt.close()
    else:
        print(f"Not enough embeddings for t-SNE visualization on {data_split} split (need >1 sample).")


    # ----- UMAP -----
    reducer = UMAP(n_components=2, random_state=42)
    if len(embeddings) > reducer.n_neighbors:
        emb_umap = reducer.fit_transform(embeddings)
        plt.figure(figsize=(8, 8))
        current_label_names = full_dataset.label_encoder.inverse_transform(np.unique(true_labels))
        for i, name in enumerate(current_label_names):
            original_encoded_label = full_dataset.label_encoder.transform([name])[0]
            idxs = true_labels == original_encoded_label
            plt.scatter(emb_umap[idxs, 0], emb_umap[idxs, 1], label=name, alpha=0.6, s=50, edgecolors='w', linewidths=0.5)
        plt.legend(fontsize="x-small", loc="best")
        plt.title(f"UMAP of CNN Embeddings{eval_title_suffix}") # MODIFIED TITLE
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"umap_embeddings{eval_title_suffix.replace(' ', '_').replace('(', '').replace(')', '')}.png")) # MODIFIED FILENAME
        plt.close()
    else:
        print(f"Not enough embeddings for UMAP visualization on {data_split} split (need >{reducer.n_neighbors} samples).")

    # ----- Save CSV for t-SNE / UMAP -----
    df_embed = pd.DataFrame(embeddings)
    df_embed['label'] = [full_dataset.label_encoder.inverse_transform([i])[0] for i in true_labels] # Ensure labels are inverse transformed correctly
    df_embed['filename'] = all_filenames
    df_embed['true_PC1'] = true_pcs[:, 0]
    df_embed['true_PC2'] = true_pcs[:, 1]
    df_embed['pred_PC1'] = pred_pcs[:, 0]
    df_embed['pred_PC2'] = pred_pcs[:, 1]
    df_embed.to_csv(os.path.join(output_dir, "embeddings.csv"), index=False)

    print(f"\n✅ Evaluation results saved in: {output_dir}")

# ====================
# Run Scripts
# ====================
if __name__ == "__main__":
    # Ensure your actual data (file_substrings, geno_labels, PCs)
    # are loaded or defined here before calling preprocess_leaf_masks.

    # These variables MUST be populated by your external script or defined here.
    # For standalone testing, you'd uncomment and use the dummy data below,
    # but for your actual workflow, these come from your data loading setup.
    
    # Example for standalone testing if you don't have your full data setup:
    num_samples_placeholder = 210
    file_substrings = [f"leaf_id_{i:03d}" for i in range(num_samples_placeholder)]
    dummy_labels_base = ["algeria", "vinifera", "rootstock", "wild", "dissected"]
    geno_labels = [random.choice(dummy_labels_base) for _ in range(num_samples_placeholder)]
    PCs = np.random.rand(num_samples_placeholder, 2) * 2 - 1
    print("\n--- WARNING: Using dummy data for standalone execution. Please replace with your actual data. ---")

    # --- Step 1: Preprocess your coordinate data to generate masks and metadata ---
    # Call this with your actual `file_substrings`, `geno_labels`, and `PCs`
    preprocess_leaf_masks(
        data_folder="data",
        PCs=PCs, 
        names=file_substrings,
        labels=geno_labels,
        resolution=512,
        n_rotations=20,
        normalize_pcs=True
    )

    # --- Step 2: Train the model ---
    trained_model, train_losses_history, val_losses_history = train_model(
        data_dir="PIXEL_IMAGES",
        batch_size=16,
        lr=3e-4, 
        epochs=150, 
        alpha=0.25 # CHANGED: Lowered alpha again
    )

    # --- Step 3: Evaluate the trained model ---
    # Evaluate on the full dataset (as before)
    evaluate_model("PIXEL_IMAGES", data_split='all') # Explicitly state data_split

    # Evaluate on the validation data only (NEW)
    evaluate_model("PIXEL_IMAGES", data_split='validation') # Explicitly state data_split