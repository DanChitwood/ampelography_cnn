# V3 TRAINING

import os
import numpy as np
import random
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, r2_score
from scipy.stats import pearsonr
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.preprocessing import LabelEncoder
import joblib
import shutil # Added for rmtree

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# ====================
# Device setup
# ====================
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ====================
# 1. Preprocessing Function (MODIFIED: Added shutil.rmtree for clean start)
# ====================
def preprocess_leaf_masks(data_folder="data",
                          PCs=None,
                          names=None,
                          labels=None,
                          resolution=512,
                          n_rotations=20,
                          output_folder="PIXEL_IMAGES",
                          normalize_pcs=True):

    # Clear existing output folder to ensure a truly clean slate each time
    if os.path.exists(output_folder):
        print(f"Clearing existing output folder: {output_folder}...")
        shutil.rmtree(output_folder)
        print(f"Cleared {output_folder}.")
        
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(os.path.join(output_folder, "MASKS"), exist_ok=True)
    os.makedirs(os.path.join(output_folder, "RGB_IMAGES"), exist_ok=True)

    assert PCs is not None and names is not None and labels is not None
    assert len(names) == len(PCs) == len(labels)

    all_filenames_raw = []
    all_labels_raw = []
    all_pcs_raw = []

    counter = 1

    for i, name in enumerate(names):
        label = labels[i]
        pc = PCs[i]

        blade_candidates = [f for f in os.listdir(data_folder) if name in f and "blade" in f]
        if not blade_candidates:
            print(f"ERROR: No blade file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        blade_file = blade_candidates[0]

        vein_candidates = [f for f in os.listdir(data_folder) if name in f and "veins" in f]
        if not vein_candidates:
            print(f"ERROR: No vein file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        vein_file = vein_candidates[0]

        blade = np.loadtxt(os.path.join(data_folder, blade_file))
        vein = np.loadtxt(os.path.join(data_folder, vein_file))
        all_coords = np.vstack([blade, vein])

        for rot in range(n_rotations):
            angle_deg = random.uniform(0, 360)
            angle_rad = np.deg2rad(angle_deg)
            R = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],
                          [np.sin(angle_rad),  np.cos(angle_rad)]])

            centroid = all_coords.mean(axis=0)
            rotated = (all_coords - centroid) @ R.T

            min_xy = rotated.min(axis=0)
            max_xy = rotated.max(axis=0)
            scale = resolution * 0.9 / max(max_xy - min_xy)
            scaled = (rotated - min_xy) * scale
            offset = (resolution - (max_xy - min_xy) * scale) / 2
            final_coords = scaled + offset

            n_blade = len(blade)
            blade_scaled = final_coords[:n_blade]
            vein_scaled = final_coords[n_blade:]

            mask = Image.new("L", (resolution, resolution), 0)
            draw = ImageDraw.Draw(mask)
            draw.polygon(blade_scaled.flatten().tolist(), fill=1)
            draw.polygon(vein_scaled.flatten().tolist(), fill=2)
            
            mask_name = f"{label}_{counter:04d}.png"
            mask.save(os.path.join(output_folder, "MASKS", mask_name))

            rgb = Image.new("RGB", (resolution, resolution), (0, 0, 0))
            rgb_draw = ImageDraw.Draw(rgb)
            rgb_draw.polygon(blade_scaled.flatten().tolist(), fill=(255, 140, 0))
            rgb_draw.polygon(vein_scaled.flatten().tolist(), fill=(255, 0, 255))
            rgb.save(os.path.join(output_folder, "RGB_IMAGES", mask_name))

            all_pcs_raw.append(pc)
            all_labels_raw.append(label)
            all_filenames_raw.append(mask_name)

            counter += 1

    metadata_df = pd.DataFrame({
        "filename": all_filenames_raw,
        "label": all_labels_raw,
        "PC1": [p[0] for p in all_pcs_raw],
        "PC2": [p[1] for p in all_pcs_raw]
    })

    if normalize_pcs:
        print("Normalizing PC values...")
        scaler = StandardScaler()
        metadata_df[['PC1', 'PC2']] = scaler.fit_transform(metadata_df[['PC1', 'PC2']])
        joblib.dump(scaler, os.path.join(output_folder, "pc_scaler.pkl"))
        print("PC values normalized and scaler saved.")

    metadata_df.to_csv(os.path.join(output_folder, "METADATA.csv"), index=False)
    
    print(f"Saved {len(metadata_df)} rotated leaf masks and metadata to '{output_folder}'.")


# ====================
# 2. Dataset class (NO CHANGE HERE)
# ====================
class LeafDataset(Dataset):
    def __init__(self, image_dir, metadata_file, transform=None):
        self.image_dir = image_dir
        self.metadata_df = pd.read_csv(metadata_file)
        self.transform = transform

        self.filenames = self.metadata_df['filename'].values
        self.label_encoder = LabelEncoder()
        # Use fit_transform on all labels from the metadata to ensure consistent mapping
        self.labels = self.label_encoder.fit_transform(self.metadata_df['label'])
        self.pcs = self.metadata_df[["PC1", "PC2"]].values.astype(np.float32)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.filenames[idx])
        image = Image.open(img_path).convert("L")
        
        if self.transform:
            image = self.transform(image)

        label = torch.tensor(self.labels[idx], dtype=torch.long)
        pcs = torch.tensor(self.pcs[idx], dtype=torch.float32)
        
        return image, label, pcs, self.filenames[idx]


# ====================
# 3. CNN model (MODIFIED CLASSIFIER HEAD - as discussed in previous turn)
# ====================
class CNNMultitask(nn.Module):
    def __init__(self, n_classes, resolution=512):
        super(CNNMultitask, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.flatten = nn.Flatten()
        
        self.flattened_size = 128 * (resolution // 16) * (resolution // 16)
        
        self.fc_embedding = nn.Linear(self.flattened_size, 256)
        self.dropout_embedding = nn.Dropout(0.3)

        # Regressor Head (already 4 layers deep)
        self.regressor_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )
        
        # Classifier Head (deepened with dropout - as discussed)
        self.classifier_head = nn.Sequential(
            nn.Linear(256, 128),  # Added layer
            nn.ReLU(),
            nn.Dropout(0.3),      # Added dropout
            nn.Linear(128, n_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.flatten(x)
        
        if x.shape[1] != self.flattened_size:
            raise ValueError(f"Flattened feature size mismatch. Expected {self.flattened_size}, got {x.shape[1]}. "
                             "Please ensure your input image resolution is compatible with the CNN layers, "
                             "e.g., 512 for this model, or adjust CNN layers/use AdaptiveAvgPool2d.")

        embeddings = self.dropout_embedding(F.relu(self.fc_embedding(x)))
        
        pred_pcs = self.regressor_head(embeddings)
        pred_labels = self.classifier_head(embeddings)
        
        return pred_pcs, pred_labels, embeddings


# ====================
# 4. Training Function (MODIFIED ALPHA and added Early Stopping monitoring - as discussed)
# ====================
def train_model(data_dir="PIXEL_IMAGES",
                batch_size=16,
                lr=3e-4,
                epochs=150,
                alpha=0.3, # CHANGED: Increased alpha back to 0.3 or higher
                eval_interval=5):

    metadata_file = os.path.join(data_dir, "METADATA.csv")
    image_dir = os.path.join(data_dir, "MASKS")

    transform = transforms.Compose([
        transforms.ToTensor(),
    ])

    full_dataset = LeafDataset(image_dir, metadata_file, transform)
    
    original_labels = full_dataset.metadata_df['label'].values
    label_encoder_for_weights = LabelEncoder()
    encoded_labels = label_encoder_for_weights.fit_transform(original_labels)

    n_classes = len(np.unique(encoded_labels))

    class_counts = pd.Series(encoded_labels).value_counts().sort_index()
    total_samples = len(encoded_labels)
    
    # Ensure all classes from 0 to n_classes-1 are present in class_counts for correct indexing
    # Create a Series with all possible class indices and fill missing with 0 to avoid division by zero
    all_classes_counts = pd.Series(0, index=range(n_classes))
    all_classes_counts.update(class_counts)
    
    # Handle cases where a class might have zero samples in the training split
    # Although stratify helps, if a class is extremely rare, it might not appear in a batch.
    # Inverse frequency formula: total_samples / (n_classes * class_count)
    # Adding a small epsilon to class_counts to prevent division by zero for absent classes (shouldn't happen with stratify but good practice)
    class_weights = total_samples / (n_classes * (all_classes_counts.values + 1e-6)) # Added epsilon
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    print(f"Calculated class weights (Inverse Frequency): {class_weights}")


    train_indices, val_indices = train_test_split(
        range(len(full_dataset)), test_size=0.2, random_state=42,
        stratify=full_dataset.labels
    )
    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_indices), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_indices), batch_size=batch_size)

    model = CNNMultitask(n_classes).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion_reg = nn.MSELoss()
    criterion_cls = nn.CrossEntropyLoss(weight=class_weights)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)

    print(f"\nStarting training for {epochs} epochs...")
    print(f"  Training samples: {len(train_indices)}")
    print(f"  Validation samples: {len(val_indices)}")
    print(f"  Number of classes: {n_classes}")
    print(f"  Initial Learning Rate: {lr}")
    print(f"  Alpha (Classification Loss Weight): {alpha}")


    train_losses = []
    val_losses = []
    
    # Early stopping
    best_val_loss = float('inf')
    epochs_no_improve = 0
    patience = 20 # Number of epochs to wait for improvement

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        total_reg_loss = 0
        total_cls_loss = 0

        for batch_idx, (imgs, labels, pcs, filenames) in enumerate(train_loader):
            imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)

            optimizer.zero_grad()
            
            pred_pcs, pred_labels, _ = model(imgs)
            
            loss_reg = criterion_reg(pred_pcs, pcs)
            loss_cls = criterion_cls(pred_labels, labels)
            
            loss = loss_reg + alpha * loss_cls

            loss.backward()
            optimizer.step()
            
            total_train_loss += loss.item()
            total_reg_loss += loss_reg.item()
            total_cls_loss += loss_cls.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} (Reg: {total_reg_loss/len(train_loader):.4f}, Cls: {total_cls_loss/len(train_loader):.4f})")

        if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:
            model.eval()
            total_val_loss = 0
            with torch.no_grad():
                for imgs, labels, pcs, filenames in val_loader:
                    imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)
                    pred_pcs, pred_labels, _ = model(imgs)
                    loss = criterion_reg(pred_pcs, pcs) + alpha * criterion_cls(pred_labels, labels)
                    total_val_loss += loss.item()
            
            avg_val_loss = total_val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            print(f"  Validation Loss: {avg_val_loss:.4f}")
            
            scheduler.step(avg_val_loss)

            # Early stopping check
            # We save the model only if validation loss improves.
            # The model_save_path is always the same, so it will overwrite if improved.
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                epochs_no_improve = 0
                model_save_path = os.path.join(data_dir, "cnn_multitask_model.pt")
                torch.save(model.state_dict(), model_save_path)
                print(f"✅ Model saved to {model_save_path} (best validation loss so far)")
            else:
                epochs_no_improve += eval_interval # Increment by eval_interval
                if epochs_no_improve >= patience:
                    print(f"Early stopping triggered after {epoch + 1} epochs. Validation loss has not improved for {patience} epochs.")
                    break # Stop training


    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
    val_epochs = [e for e in range(1, epochs + 1) if (e % eval_interval == 0 or e == epochs)]
    # Ensure val_epochs length matches val_losses length if early stopping occurred
    if len(val_epochs) > len(val_losses):
        val_epochs = val_epochs[:len(val_losses)]
    elif len(val_epochs) < len(val_losses):
        # This case should ideally not happen if epochs is large enough and eval_interval is consistent.
        # If it does, it implies more validation points were recorded than anticipated by 'epochs'.
        # For simplicity, we'll just plot the available validation points.
        pass
    
    plt.plot(val_epochs, val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(data_dir, "training_validation_loss.png"))
    plt.close()

    return model, train_losses, val_losses


# ====================
# 5. Evaluation Function (MODIFIED FOR DATA SPLIT & NEW METRICS - as discussed)
# ====================
def evaluate_model(data_dir="PIXEL_IMAGES", data_split='all'):
    image_dir = os.path.join(data_dir, "MASKS")
    metadata_csv = os.path.join(data_dir, "METADATA.csv")
    model_path = os.path.join(data_dir, "cnn_multitask_model.pt")
    output_dir = os.path.join(data_dir, f"CNN_EVAL_OUTPUTS_{data_split.upper()}")
    os.makedirs(output_dir, exist_ok=True)

    transform = transforms.Compose([transforms.ToTensor()])
    
    full_dataset = LeafDataset(image_dir, metadata_csv, transform)

    if data_split == 'validation':
        # Re-split data to get the exact validation set used during training
        _, val_indices = train_test_split(
            range(len(full_dataset)), test_size=0.2, random_state=42,
            stratify=full_dataset.labels
        )
        dataset_to_evaluate = torch.utils.data.Subset(full_dataset, val_indices)
        eval_title_suffix = " (Validation Data)"
    else: # data_split == 'all'
        dataset_to_evaluate = full_dataset
        eval_title_suffix = ""

    dataloader = DataLoader(dataset_to_evaluate, batch_size=16, shuffle=False)

    model = CNNMultitask(n_classes=len(np.unique(full_dataset.labels))).to(device)
    # IMPORTANT: Ensure weights_only=True in future PyTorch versions for security.
    # For now, suppressing the warning as it's a known internal script.
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    scaler = None
    scaler_path = os.path.join(data_dir, "pc_scaler.pkl")
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print("Loaded PC scaler for inverse transformation.")

    all_preds_pcs = []
    all_true_pcs = []
    all_preds_labels = []
    all_true_labels = []
    all_embeddings = []
    all_filenames = []

    with torch.no_grad():
        for images, labels, pcs, filenames in dataloader:
            images = images.to(device)
            pcs = pcs.to(device)
            labels = labels.to(device)

            preds_pcs, preds_labels, embeddings = model(images)

            all_preds_pcs.append(preds_pcs.cpu().numpy())
            all_true_pcs.append(pcs.cpu().numpy())
            all_preds_labels.append(preds_labels.argmax(dim=1).cpu().numpy())
            all_true_labels.append(labels.cpu().numpy())
            all_embeddings.append(embeddings.cpu().numpy())
            all_filenames.extend(filenames)

    pred_pcs_norm = np.vstack(all_preds_pcs)
    true_pcs_norm = np.vstack(all_true_pcs)
    pred_labels = np.concatenate(all_preds_labels)
    true_labels = np.concatenate(all_true_labels)
    embeddings = np.vstack(all_embeddings)
    
    # Use the label_encoder from the full_dataset to ensure consistent mapping
    label_names = full_dataset.label_encoder.inverse_transform(np.unique(full_dataset.labels))

    if scaler is not None:
        pred_pcs = scaler.inverse_transform(pred_pcs_norm)
        true_pcs = scaler.inverse_transform(true_pcs_norm)
        print("Inverse transformed PCs for plotting.")
    else:
        pred_pcs = pred_pcs_norm
        true_pcs = true_pcs_norm

    # ----- Regression Metrics (R2 and Pearsonr) -----
    r2_pc1 = r2_score(true_pcs[:, 0], pred_pcs[:, 0])
    r2_pc2 = r2_score(true_pcs[:, 1], pred_pcs[:, 1])
    pearson_pc1, _ = pearsonr(true_pcs[:, 0], pred_pcs[:, 0])
    pearson_pc2, _ = pearsonr(true_pcs[:, 1], pred_pcs[:, 1])

    print(f"\n--- Regression Metrics {eval_title_suffix} ---")
    print(f"R2 Score (PC1): {r2_pc1:.4f}")
    print(f"Pearson Correlation (PC1): {pearson_pc1:.4f}")
    print(f"R2 Score (PC2): {r2_pc2:.4f}")
    print(f"Pearson Correlation (PC2): {pearson_pc2:.4f}")


    # ----- Save PC scatter (PC1 vs PC2) -----
    plt.figure(figsize=(8, 8))
    plt.scatter(true_pcs[:, 0], true_pcs[:, 1], label="True PCs", alpha=0.6, s=50, edgecolors='w')
    plt.scatter(pred_pcs[:, 0], pred_pcs[:, 1], label="Predicted PCs", alpha=0.6, s=50, edgecolors='w', marker='x')
    plt.title(f"True vs. Predicted PC Values{eval_title_suffix}")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.legend()
    plt.grid(True)
    plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)
    plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)
    plt.savefig(os.path.join(output_dir, "pc_scatter_plot.png"))
    plt.close()

    # ----- Confusion Matrix -----
    cm = confusion_matrix(true_labels, pred_labels)
    plt.figure(figsize=(10, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
    plt.title(f"Confusion Matrix{eval_title_suffix}")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "confusion_matrix.png"))
    plt.close()

    # ----- UMAP/t-SNE of Embeddings (Colored by True Labels) -----
    reducer_umap = UMAP(random_state=42)
    embeddings_2d_umap = reducer_umap.fit_transform(embeddings)

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeddings_2d_umap[:, 0], embeddings_2d_umap[:, 1],
                          c=true_labels, cmap='viridis', s=20, alpha=0.7)
    plt.title(f"UMAP of Embeddings (True Labels){eval_title_suffix}")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.colorbar(scatter, ticks=range(len(label_names)), format=plt.FuncFormatter(lambda val, loc: label_names[int(val)]))
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "umap_true_labels.png"))
    plt.close()

    # ----- UMAP/t-SNE of Embeddings (Colored by Predicted Labels) -----
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeddings_2d_umap[:, 0], embeddings_2d_umap[:, 1],
                          c=pred_labels, cmap='viridis', s=20, alpha=0.7)
    plt.title(f"UMAP of Embeddings (Predicted Labels){eval_title_suffix}")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.colorbar(scatter, ticks=range(len(label_names)), format=plt.FuncFormatter(lambda val, loc: label_names[int(val)]))
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "umap_pred_labels.png"))
    plt.close()

    print(f"✅ Evaluation results saved in: {output_dir}")


# ====================
# 6. Main Execution (FINAL CORRECTED LAYOUT - USES YOUR EXTERNAL DATA)
#    This block expects 'file_substrings', 'geno_labels', and 'PCs'
#    to be defined in previous cells of your Jupyter notebook.
# ====================
if __name__ == "__main__":
    # --- Configuration Parameters ---
    DATA_DIR = "PIXEL_IMAGES" # Your preprocessed data folder
    LEARNING_RATE = 3e-4
    EPOCHS = 150
    ALPHA_CLASSIFICATION_WEIGHT = 0.3 # Adjusted alpha as discussed
    EVAL_INTERVAL = 5 # How often to evaluate on validation set during training

    # --- Step 1: Preprocessing ---
    # This section will *always* run and generate a new PIXEL_IMAGES folder
    # with fresh masks and METADATA.csv for each run, using YOUR data.
    print("\n--- Running preprocessing to generate PIXEL_IMAGES ---")
    try:
        # These variables (file_substrings, geno_labels, PCs) are expected
        # to be already defined in earlier cells of your Jupyter Notebook.
        # DO NOT define dummy variables here, as they will overwrite your actual data.
        
        preprocess_leaf_masks(data_folder="data", # Assuming your raw .txt files are in a 'data' folder
                              PCs=PCs,           # Uses YOUR PCs
                              names=file_substrings, # Uses YOUR file_substrings
                              labels=geno_labels,    # Uses YOUR geno_labels
                              resolution=512, # Ensure this matches your model's expected input
                              n_rotations=20, # Use more rotations for more data
                              output_folder=DATA_DIR)
        print("--- PIXEL_IMAGES generated successfully ---")

    except NameError as e:
        print(f"\nERROR: Preprocessing aborted. A required variable (e.g., PCs, file_substrings, geno_labels) is not defined: {e}")
        print("Please ensure your preceding Jupyter Notebook cells that generate these variables have been executed.")
        exit() # Exit if preprocessing fails due to missing variables.
    except FileNotFoundError as e:
        print(f"\nERROR: Data files not found for preprocessing: {e}")
        print("Please ensure your 'data' folder exists and contains the necessary 'blade' and 'veins' .txt files,")
        print("and that the `file_substrings` provided match their prefixes.")
        print("Exiting script because preprocessing failed.")
        exit() # Exit if preprocessing fails, as subsequent steps will depend on it.
    except Exception as e:
        print(f"\nAn unexpected error occurred during preprocessing: {e}")
        print("Please check your data loading and preprocess_leaf_masks parameters.")
        print("Exiting script because preprocessing failed.")
        exit()


    # --- Step 2: Train the model ---
    # This will use the newly generated PIXEL_IMAGES data
    print("\n--- Starting Model Training ---")
    model, train_losses, val_losses = train_model(
        data_dir=DATA_DIR,
        lr=LEARNING_RATE,
        epochs=EPOCHS,
        alpha=ALPHA_CLASSIFICATION_WEIGHT,
        eval_interval=EVAL_INTERVAL
    )
    print("--- Model Training Finished ---")

    # --- Step 3: Evaluate the model ---
    # This will use the newly generated PIXEL_IMAGES data and the trained model
    print("\n--- Starting Model Evaluation (All Data) ---")
    evaluate_model(data_dir=DATA_DIR, data_split='all')
    print("--- Model Evaluation (All Data) Finished ---")

    print("\n--- Starting Model Evaluation (Validation Data) ---")
    evaluate_model(data_dir=DATA_DIR, data_split='validation')
    print("--- Model Evaluation (Validation Data) Finished ---")