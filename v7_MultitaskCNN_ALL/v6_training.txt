import os
import numpy as np
import random
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    r2_score, precision_recall_fscore_support, accuracy_score
)
from scipy.stats import pearsonr
from umap import UMAP
import joblib
import shutil

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# ====================
# Device setup
# ====================
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ====================
# 1. Preprocessing Function (MODIFIED: Noise augmentation updated)
# ====================
def preprocess_leaf_masks(data_folder="data",
                          PCs=None,
                          names=None,
                          labels=None,
                          resolution=512,
                          n_rotations=20,
                          output_folder="PIXEL_IMAGES",
                          normalize_pcs=True,
                          blade_noise_prop_range=(0.0, 0.2), # MODIFIED: Increased to 0.2
                          vein_noise_prop_range=(0.0, 1.0),
                          blade_noise_num_clumps=(1, 5)): # NEW: Number of clumps/seeds for blade noise

    # Clear existing output folder to ensure a truly clean slate each time
    if os.path.exists(output_folder):
        print(f"Clearing existing output folder: {output_folder}...")
        shutil.rmtree(output_folder)
        print(f"Cleared {output_folder}.")

    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(os.path.join(output_folder, "MASKS"), exist_ok=True)
    os.makedirs(os.path.join(output_folder, "RGB_IMAGES"), exist_ok=True)

    assert PCs is not None and names is not None and labels is not None
    assert len(names) == len(PCs) == len(labels)

    all_filenames_raw = []
    all_labels_raw = []
    all_pcs_raw = []

    counter = 1

    for i, name in enumerate(names):
        label = labels[i]
        pc = PCs[i]

        blade_candidates = [f for f in os.listdir(data_folder) if name in f and "blade" in f]
        if not blade_candidates:
            print(f"ERROR: No blade file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        blade_file = blade_candidates[0]

        vein_candidates = [f for f in os.listdir(data_folder) if name in f and "veins" in f]
        if not vein_candidates:
            print(f"ERROR: No vein file found for '{name}' in '{data_folder}'. Skipping this sample.")
            continue
        vein_file = vein_candidates[0]

        blade = np.loadtxt(os.path.join(data_folder, blade_file))
        vein = np.loadtxt(os.path.join(data_folder, vein_file))
        all_coords_original = np.vstack([blade, vein]) # Keep original for centroid calculation

        # Calculate centroid once per leaf, before rotations
        leaf_centroid = all_coords_original.mean(axis=0)

        for rot in range(n_rotations):
            angle_deg = random.uniform(0, 360)
            angle_rad = np.deg2rad(angle_deg)
            R = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],
                          [np.sin(angle_rad),  np.cos(angle_rad)]])

            # Apply rotation around centroid
            rotated = (all_coords_original - leaf_centroid) @ R.T + leaf_centroid # Rotate around original centroid

            min_xy = rotated.min(axis=0)
            max_xy = rotated.max(axis=0)
            scale = resolution * 0.9 / max(max_xy - min_xy)
            scaled = (rotated - min_xy) * scale
            offset = (resolution - (max_xy - min_xy) * scale) / 2
            final_coords = scaled + offset

            n_blade = len(blade)
            blade_scaled = final_coords[:n_blade]
            vein_scaled = final_coords[n_blade:]

            mask = Image.new("L", (resolution, resolution), 0)
            draw = ImageDraw.Draw(mask)
            draw.polygon(blade_scaled.flatten().tolist(), fill=1) # Blade
            draw.polygon(vein_scaled.flatten().tolist(), fill=2)  # Vein

            # Convert mask to numpy array for pixel manipulation
            mask_np = np.array(mask)

            # --- Apply Noise ---
            # 1. Vein pixel removal (replace with blade) with Gaussian probability
            vein_pixels_coords = np.argwhere(mask_np == 2)
            if len(vein_pixels_coords) > 0:
                prop_to_remove_vein = random.uniform(*vein_noise_prop_range)
                num_to_remove_vein = int(prop_to_remove_vein * len(vein_pixels_coords))

                if num_to_remove_vein > 0:
                    # Calculate distances from centroid for vein pixels in image coordinates
                    # Scaled centroid: Re-calculate based on current rotation's scaled coordinates
                    current_scaled_centroid = (leaf_centroid - min_xy) * scale + offset

                    vein_distances = np.linalg.norm(vein_pixels_coords - current_scaled_centroid.round().astype(int), axis=1)

                    if vein_distances.max() > 0:
                        removal_probabilities = vein_distances / vein_distances.max() # Higher distance = higher prob
                    else:
                        removal_probabilities = np.zeros_like(vein_distances)

                    # Select pixels to remove based on these probabilities
                    # Sort indices by their removal probability in descending order and pick the top ones.
                    sorted_indices = np.argsort(removal_probabilities)[::-1]
                    indices_to_remove_actual = sorted_indices[:num_to_remove_vein]

                    for idx_in_list in indices_to_remove_actual:
                        r, c = vein_pixels_coords[idx_in_list]
                        mask_np[r, c] = 1 # MODIFIED: Replace vein (2) with blade (1)


            # 2. Blade pixel removal (clumped, replace with background)
            blade_pixels_coords = np.argwhere(mask_np == 1)
            if len(blade_pixels_coords) > 0:
                prop_to_remove_blade = random.uniform(*blade_noise_prop_range)
                num_to_remove_blade = int(prop_to_remove_blade * len(blade_pixels_coords))

                if num_to_remove_blade > 0:
                    num_clumps = random.randint(*blade_noise_num_clumps)
                    
                    # Ensure we have enough blade pixels to form distinct clumps
                    if len(blade_pixels_coords) < num_clumps:
                         num_clumps = len(blade_pixels_coords) # Limit clumps to available pixels

                    # Randomly select clump centers (seeds) from existing blade pixels
                    clump_centers_indices = random.sample(range(len(blade_pixels_coords)), num_clumps)
                    clump_centers = blade_pixels_coords[clump_centers_indices]

                    removed_pixels_count = 0
                    pixels_to_remove_set = set() # Use a set to avoid duplicates

                    # Iteratively remove pixels around centers until target count is met
                    # Or, more robustly, calculate a probability for each pixel based on distance to nearest clump center
                    
                    # Calculate distance of each blade pixel to its nearest clump center
                    min_distances = np.full(len(blade_pixels_coords), np.inf)
                    for center in clump_centers:
                        distances_to_center = np.linalg.norm(blade_pixels_coords - center, axis=1)
                        min_distances = np.minimum(min_distances, distances_to_center)

                    # Normalize distances (0 = furthest from any clump center, 1 = closest)
                    # We want to remove pixels *closest* to the clump centers, so invert the probability
                    if min_distances.max() > 0:
                        removal_probabilities = 1 - (min_distances / min_distances.max())
                    else:
                        removal_probabilities = np.zeros_like(min_distances)

                    # Select pixels based on these probabilities
                    sorted_indices = np.argsort(removal_probabilities)[::-1] # Sort descending (highest prob first)
                    
                    for idx_in_list in sorted_indices:
                        if removed_pixels_count >= num_to_remove_blade:
                            break
                        r, c = blade_pixels_coords[idx_in_list]
                        # Check if it's still a blade pixel (hasn't been removed by another clump)
                        if mask_np[r,c] == 1:
                            mask_np[r, c] = 0 # Set to background
                            removed_pixels_count += 1
            # --- End Apply Noise ---

            # Convert back to PIL Image and save
            mask = Image.fromarray(mask_np)
            mask_name = f"{label}_{counter:04d}.png"
            mask.save(os.path.join(output_folder, "MASKS", mask_name))

            rgb = Image.new("RGB", (resolution, resolution), (0, 0, 0))
            # Re-draw based on potentially modified mask_np for accurate visualization
            for r in range(resolution):
                for c in range(resolution):
                    if mask_np[r,c] == 1:
                        rgb.putpixel((c,r), (255, 140, 0)) # Blade orange
                    elif mask_np[r,c] == 2: # Veins (if any remained after removal/replacement)
                        rgb.putpixel((c,r), (255, 0, 255)) # Vein magenta
            rgb.save(os.path.join(output_folder, "RGB_IMAGES", mask_name))


            all_pcs_raw.append(pc)
            all_labels_raw.append(label)
            all_filenames_raw.append(mask_name)

            counter += 1

    metadata_df = pd.DataFrame({
        "filename": all_filenames_raw,
        "label": all_labels_raw,
        "PC1": [p[0] for p in all_pcs_raw],
        "PC2": [p[1] for p in all_pcs_raw]
    })

    if normalize_pcs:
        print("Normalizing PC values...")
        scaler = StandardScaler()
        metadata_df[['PC1', 'PC2']] = scaler.fit_transform(metadata_df[['PC1', 'PC2']])
        joblib.dump(scaler, os.path.join(output_folder, "pc_scaler.pkl"))
        print("PC values normalized and scaler saved.")

    metadata_df.to_csv(os.path.join(output_folder, "METADATA.csv"), index=False)

    print(f"Saved {len(metadata_df)} rotated leaf masks and metadata to '{output_folder}'.")


# ====================
# 2. Dataset class (NO CHANGE HERE)
# ====================
class LeafDataset(Dataset):
    def __init__(self, image_dir, metadata_file, transform=None):
        self.image_dir = image_dir
        self.metadata_df = pd.read_csv(metadata_file)
        self.transform = transform

        self.filenames = self.metadata_df['filename'].values
        self.label_encoder = LabelEncoder()
        # Use fit_transform on all labels from the metadata to ensure consistent mapping
        self.labels = self.label_encoder.fit_transform(self.metadata_df['label'])
        self.pcs = self.metadata_df[["PC1", "PC2"]].values.astype(np.float32)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.filenames[idx])
        image = Image.open(img_path).convert("L")

        if self.transform:
            image = self.transform(image)

        label = torch.tensor(self.labels[idx], dtype=torch.long)
        pcs = torch.tensor(self.pcs[idx], dtype=torch.float32)

        return image, label, pcs, self.filenames[idx]


# ====================
# 3. CNN model (NO CHANGE HERE)
# ====================
class CNNMultitask(nn.Module):
    def __init__(self, n_classes, resolution=512):
        super(CNNMultitask, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.flatten = nn.Flatten()

        self.flattened_size = 128 * (resolution // 16) * (resolution // 16)

        self.fc_embedding = nn.Linear(self.flattened_size, 256)
        self.dropout_embedding = nn.Dropout(0.3)

        # Regressor Head (already 4 layers deep)
        self.regressor_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

        # Classifier Head (deepened with dropout)
        self.classifier_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, n_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.flatten(x)

        if x.shape[1] != self.flattened_size:
            raise ValueError(f"Flattened feature size mismatch. Expected {self.flattened_size}, got {x.shape[1]}. "
                             "Please ensure your input image resolution is compatible with the CNN layers, "
                             "e.g., 512 for this model, or adjust CNN layers/use AdaptiveAvgPool2d.")

        embeddings = self.dropout_embedding(F.relu(self.fc_embedding(x)))

        pred_pcs = self.regressor_head(embeddings)
        pred_labels = self.classifier_head(embeddings)

        return pred_pcs, pred_labels, embeddings


# ====================
# 4. Training Function (NO CHANGE HERE)
# ====================
def train_model(data_dir="PIXEL_IMAGES",
                batch_size=16,
                lr=3e-4,
                epochs=150,
                alpha=0.3,
                eval_interval=5):

    metadata_file = os.path.join(data_dir, "METADATA.csv")
    image_dir = os.path.join(data_dir, "MASKS")

    transform = transforms.Compose([
        transforms.ToTensor(),
    ])

    full_dataset = LeafDataset(image_dir, metadata_file, transform)

    original_labels = full_dataset.metadata_df['label'].values
    label_encoder_for_weights = LabelEncoder()
    encoded_labels = label_encoder_for_weights.fit_transform(original_labels)

    n_classes = len(np.unique(encoded_labels))

    class_counts = pd.Series(encoded_labels).value_counts().sort_index()
    total_samples = len(encoded_labels)

    # Ensure all classes from 0 to n_classes-1 are present in class_counts for correct indexing
    all_classes_counts = pd.Series(0, index=range(n_classes))
    all_classes_counts.update(class_counts)

    # Inverse frequency class weights
    class_weights = total_samples / (n_classes * (all_classes_counts.values + 1e-6))
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    print(f"Calculated class weights (Inverse Frequency): {class_weights}")


    train_indices, val_indices = train_test_split(
        range(len(full_dataset)), test_size=0.2, random_state=42,
        stratify=full_dataset.labels
    )
    train_loader = DataLoader(torch.utils.data.Subset(full_dataset, train_indices), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(torch.utils.data.Subset(full_dataset, val_indices), batch_size=batch_size)

    model = CNNMultitask(n_classes).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion_reg = nn.MSELoss()
    criterion_cls = nn.CrossEntropyLoss(weight=class_weights)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)

    print(f"\nStarting training for {epochs} epochs...")
    print(f"  Training samples: {len(train_indices)}")
    print(f"  Validation samples: {len(val_indices)}")
    print(f"  Number of classes: {n_classes}")
    print(f"  Initial Learning Rate: {lr}")
    print(f"  Alpha (Classification Loss Weight): {alpha}")


    train_losses = []
    val_losses = []

    # Early stopping
    best_val_loss = float('inf')
    epochs_no_improve = 0
    patience = 20

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        total_reg_loss = 0
        total_cls_loss = 0

        for batch_idx, (imgs, labels, pcs, filenames) in enumerate(train_loader):
            imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)

            optimizer.zero_grad()

            pred_pcs, pred_labels, _ = model(imgs)

            loss_reg = criterion_reg(pred_pcs, pcs)
            loss_cls = criterion_cls(pred_labels, labels)

            loss = loss_reg + alpha * loss_cls

            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()
            total_reg_loss += loss_reg.item()
            total_cls_loss += loss_cls.item()

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} (Reg: {total_reg_loss/len(train_loader):.4f}, Cls: {total_cls_loss/len(train_loader):.4f})")

        if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:
            model.eval()
            total_val_loss = 0
            with torch.no_grad():
                for imgs, labels, pcs, filenames in val_loader:
                    imgs, labels, pcs = imgs.to(device), labels.to(device), pcs.to(device)
                    pred_pcs, pred_labels, _ = model(imgs)
                    loss = criterion_reg(pred_pcs, pcs) + alpha * criterion_cls(pred_labels, labels)
                    total_val_loss += loss.item()

            avg_val_loss = total_val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            print(f"  Validation Loss: {avg_val_loss:.4f}")

            scheduler.step(avg_val_loss)

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                epochs_no_improve = 0
                model_save_path = os.path.join(data_dir, "cnn_multitask_model.pt")
                torch.save(model.state_dict(), model_save_path)
                print(f"✅ Model saved to {model_save_path} (best validation loss so far)")
            else:
                epochs_no_improve += eval_interval
                if epochs_no_improve >= patience:
                    print(f"Early stopping triggered after {epoch + 1} epochs. Validation loss has not improved for {patience} epochs.")
                    break


    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
    val_epochs = [e for e in range(1, epochs + 1) if (e % eval_interval == 0 or e == epochs)]
    if len(val_epochs) > len(val_losses):
        val_epochs = val_epochs[:len(val_losses)]
    elif len(val_epochs) < len(val_losses):
        pass # Handle cases where val_epochs might be shorter due to early stopping

    plt.plot(val_epochs, val_losses, label='Validation Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(data_dir, "training_validation_loss.png"))
    plt.close()

    return model, train_losses, val_losses


# ====================
# 5. Evaluation Function (NO CHANGE HERE)
# ====================
def evaluate_model(data_dir="PIXEL_IMAGES", data_split='all'):
    image_dir = os.path.join(data_dir, "MASKS")
    metadata_csv = os.path.join(data_dir, "METADATA.csv")
    model_path = os.path.join(data_dir, "cnn_multitask_model.pt")
    output_dir = os.path.join(data_dir, f"CNN_EVAL_OUTPUTS_{data_split.upper()}")
    os.makedirs(output_dir, exist_ok=True)

    transform = transforms.Compose([transforms.ToTensor()])

    full_dataset = LeafDataset(image_dir, metadata_csv, transform)

    if data_split == 'validation':
        _, val_indices = train_test_split(
            range(len(full_dataset)), test_size=0.2, random_state=42,
            stratify=full_dataset.labels
        )
        dataset_to_evaluate = torch.utils.data.Subset(full_dataset, val_indices)
        eval_title_suffix = " (Validation Data)"
    else: # data_split == 'all'
        dataset_to_evaluate = full_dataset
        eval_title_suffix = ""

    dataloader = DataLoader(dataset_to_evaluate, batch_size=16, shuffle=False)

    model = CNNMultitask(n_classes=len(np.unique(full_dataset.labels))).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    scaler = None
    scaler_path = os.path.join(data_dir, "pc_scaler.pkl")
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print("Loaded PC scaler for inverse transformation.")

    all_preds_pcs = []
    all_true_pcs = []
    all_preds_labels = []
    all_true_labels = []
    all_embeddings = []
    all_filenames = []

    with torch.no_grad():
        for images, labels, pcs, filenames in dataloader:
            images = images.to(device)
            pcs = pcs.to(device)
            labels = labels.to(device)

            preds_pcs, preds_labels, embeddings = model(images)

            all_preds_pcs.append(preds_pcs.cpu().numpy())
            all_true_pcs.append(pcs.cpu().numpy())
            all_preds_labels.append(preds_labels.argmax(dim=1).cpu().numpy())
            all_true_labels.append(labels.cpu().numpy())
            all_embeddings.append(embeddings.cpu().numpy())
            all_filenames.extend(filenames)

    pred_pcs_norm = np.vstack(all_preds_pcs)
    true_pcs_norm = np.vstack(all_true_pcs)
    pred_labels = np.concatenate(all_preds_labels)
    true_labels = np.concatenate(all_true_labels)
    embeddings = np.vstack(all_embeddings)

    label_names = full_dataset.label_encoder.inverse_transform(np.unique(full_dataset.labels))
    num_classes = len(label_names)

    if scaler is not None:
        pred_pcs = scaler.inverse_transform(pred_pcs_norm)
        true_pcs = scaler.inverse_transform(true_pcs_norm)
        print("Inverse transformed PCs for plotting.")
    else:
        pred_pcs = pred_pcs_norm
        true_pcs = true_pcs_norm

    # --- Regression Metrics (R2 and Pearsonr) ---
    r2_pc1 = r2_score(true_pcs[:, 0], pred_pcs[:, 0])
    r2_pc2 = r2_score(true_pcs[:, 1], pred_pcs[:, 1])
    pearson_pc1, _ = pearsonr(true_pcs[:, 0], pred_pcs[:, 0])
    pearson_pc2, _ = pearsonr(true_pcs[:, 1], pred_pcs[:, 1])

    # --- Classification Metrics ---
    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=None, labels=np.arange(num_classes))
    # Calculate weighted averages
    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', labels=np.arange(num_classes))


    # --- Save Metrics to File ---
    metrics_file_path = os.path.join(output_dir, "metrics.txt")
    with open(metrics_file_path, 'w') as f:
        f.write(f"--- Evaluation Metrics {eval_title_suffix} ---\n\n")
        f.write(f"REGRESSION METRICS:\n")
        f.write(f"  R2 Score (PC1): {r2_pc1:.4f}\n")
        f.write(f"  Pearson Correlation (PC1): {pearson_pc1:.4f}\n")
        f.write(f"  R2 Score (PC2): {r2_pc2:.4f}\n")
        f.write(f"  Pearson Correlation (PC2): {pearson_pc2:.4f}\n\n")

        f.write(f"CLASSIFICATION METRICS:\n")
        f.write(f"  Overall Accuracy: {accuracy:.4f}\n\n")
        f.write(f"  Per-Class Metrics:\n")
        for i, class_name in enumerate(label_names):
            f.write(f"    Class '{class_name}':\n")
            f.write(f"      Precision: {precision[i]:.4f}\n")
            f.write(f"      Recall: {recall[i]:.4f}\n")
            f.write(f"      F1-Score: {f1[i]:.4f}\n")
        f.write(f"\n  Weighted Average Metrics:\n")
        f.write(f"    Precision (weighted): {weighted_precision:.4f}\n")
        f.write(f"    Recall (weighted): {weighted_recall:.4f}\n")
        f.write(f"    F1-Score (weighted): {weighted_f1:.4f}\n")
    print(f"✅ Metrics saved to: {metrics_file_path}")


    # --- Print to console as well ---
    print(f"\n--- Regression Metrics {eval_title_suffix} ---")
    print(f"R2 Score (PC1): {r2_pc1:.4f}")
    print(f"Pearson Correlation (PC1): {pearson_pc1:.4f}")
    print(f"R2 Score (PC2): {r2_pc2:.4f}")
    print(f"Pearson Correlation (PC2): {pearson_pc2:.4f}")

    print(f"\n--- Classification Metrics {eval_title_suffix} ---")
    print(f"Overall Accuracy: {accuracy:.4f}")
    print(f"\nPer-Class Metrics:")
    for i, class_name in enumerate(label_names):
        print(f"  Class '{class_name}': Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1-Score={f1[i]:.4f}")
    print(f"\nWeighted Average Metrics:")
    print(f"  Precision (weighted): {weighted_precision:.4f}")
    print(f"  Recall (weighted): {weighted_recall:.4f}")
    print(f"  F1-Score (weighted): {weighted_f1:.4f}")


    # --- Save PC scatter (PC1 vs PC2) ---
    plt.figure(figsize=(8, 8))
    plt.scatter(true_pcs[:, 0], true_pcs[:, 1], label="True PCs", alpha=0.6, s=50, edgecolors='w')
    plt.scatter(pred_pcs[:, 0], pred_pcs[:, 1], label="Predicted PCs", alpha=0.6, s=50, edgecolors='w', marker='x')
    plt.title(f"True vs. Predicted PC Values{eval_title_suffix}")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.legend()
    plt.grid(True)
    plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)
    plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)
    plt.savefig(os.path.join(output_dir, "pc_scatter_plot.png"))
    plt.close()

    # --- Save Individual PC Scatter Plots ---
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.scatter(true_pcs[:, 0], pred_pcs[:, 0], alpha=0.6, s=50, edgecolors='w')
    plt.title(f"True vs. Predicted PC1{eval_title_suffix}")
    plt.xlabel("True PC1")
    plt.ylabel("Predicted PC1")
    min_val = min(true_pcs[:, 0].min(), pred_pcs[:, 0].min())
    max_val = max(true_pcs[:, 0].max(), pred_pcs[:, 0].max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit') # Diagonal line
    plt.grid(True)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.scatter(true_pcs[:, 1], pred_pcs[:, 1], alpha=0.6, s=50, edgecolors='w')
    plt.title(f"True vs. Predicted PC2{eval_title_suffix}")
    plt.xlabel("True PC2")
    plt.ylabel("Predicted PC2")
    min_val = min(true_pcs[:, 1].min(), pred_pcs[:, 1].min())
    max_val = max(true_pcs[:, 1].max(), pred_pcs[:, 1].max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit') # Diagonal line
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "individual_pc_scatter_plots.png"))
    plt.close()


    # --- Confusion Matrix ---
    cm = confusion_matrix(true_labels, pred_labels)
    plt.figure(figsize=(10, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
    plt.title(f"Confusion Matrix{eval_title_suffix}")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "confusion_matrix.png"))
    plt.close()

    # --- UMAP of Embeddings (Colored by True Labels) ---
    reducer_umap = UMAP(random_state=42)
    embeddings_2d_umap = reducer_umap.fit_transform(embeddings)

    plt.figure(figsize=(10, 8))
    # Use different colors for each class in a categorical way
    for i, class_name in enumerate(label_names):
        indices = np.where(true_labels == i)
        plt.scatter(embeddings_2d_umap[indices, 0], embeddings_2d_umap[indices, 1],
                    label=class_name, s=20, alpha=0.7)
    plt.title(f"UMAP of Embeddings (True Labels){eval_title_suffix}")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.legend(title="True Label", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "umap_true_labels.png"))
    plt.close()

    # --- UMAP of Embeddings (Colored by Predicted Labels) ---
    plt.figure(figsize=(10, 8))
    for i, class_name in enumerate(label_names):
        indices = np.where(pred_labels == i)
        plt.scatter(embeddings_2d_umap[indices, 0], embeddings_2d_umap[indices, 1],
                    label=class_name, s=20, alpha=0.7)
    plt.title(f"UMAP of Embeddings (Predicted Labels){eval_title_suffix}")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.legend(title="Predicted Label", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "umap_pred_labels.png"))
    plt.close()

    print(f"✅ Evaluation results saved in: {output_dir}")


# ====================
# 6. Main Execution (FINAL CORRECTED LAYOUT - USES YOUR EXTERNAL DATA)
#    This block expects 'file_substrings', 'geno_labels', and 'PCs'
#    to be defined in previous cells of your Jupyter notebook.
# ====================
if __name__ == "__main__":
    # --- Configuration Parameters ---
    DATA_DIR = "PIXEL_IMAGES" # Your preprocessed data folder
    LEARNING_RATE = 2e-4  # MODIFIED: Lowered learning rate
    EPOCHS = 150
    ALPHA_CLASSIFICATION_WEIGHT = 0.4 # MODIFIED: Increased classification loss weight
    EVAL_INTERVAL = 5 # How often to evaluate on validation set during training
    N_ROTATIONS = 40 # Increased rotations for more data augmentation with noise
    BLADE_NOISE_PROPORTION_RANGE = (0.0, 0.2) # Min and max proportion of blade pixels to remove (clumped)
    VEIN_NOISE_PROPORTION_RANGE = (0.0, 1.0)  # Min and max proportion of vein pixels to remove (Gaussian-weighted)
    BLADE_NOISE_NUM_CLUMPS = (1, 5) # Min and max number of clumps for blade noise

    # --- Step 1: Preprocessing ---
    print("\n--- Running preprocessing to generate PIXEL_IMAGES ---")
    try:
        # These variables (file_substrings, geno_labels, PCs) are expected
        # to be already defined in earlier cells of your Jupyter Notebook.
        # DO NOT define dummy variables here, as they will overwrite your actual data.

        preprocess_leaf_masks(data_folder="data",
                              PCs=PCs,
                              names=file_substrings,
                              labels=geno_labels,
                              resolution=512,
                              n_rotations=N_ROTATIONS,
                              output_folder=DATA_DIR,
                              blade_noise_prop_range=BLADE_NOISE_PROPORTION_RANGE,
                              vein_noise_prop_range=VEIN_NOISE_PROPORTION_RANGE,
                              blade_noise_num_clumps=BLADE_NOISE_NUM_CLUMPS)
        print("--- PIXEL_IMAGES generated successfully ---")

    except NameError as e:
        print(f"\nERROR: Preprocessing aborted. A required variable (e.g., PCs, file_substrings, geno_labels) is not defined: {e}")
        print("Please ensure your preceding Jupyter Notebook cells that generate these variables have been executed.")
        exit()
    except FileNotFoundError as e:
        print(f"\nERROR: Data files not found for preprocessing: {e}")
        print("Please ensure your 'data' folder exists and contains the necessary 'blade' and 'veins' .txt files,")
        print("and that the `file_substrings` provided match their prefixes.")
        print("Exiting script because preprocessing failed.")
        exit()
    except Exception as e:
        print(f"\nAn unexpected error occurred during preprocessing: {e}")
        print("Please check your data loading and preprocess_leaf_masks parameters.")
        print("Exiting script because preprocessing failed.")
        exit()


    # --- Step 2: Train the model ---
    print("\n--- Starting Model Training ---")
    model, train_losses, val_losses = train_model(
        data_dir=DATA_DIR,
        lr=LEARNING_RATE,
        epochs=EPOCHS,
        alpha=ALPHA_CLASSIFICATION_WEIGHT,
        eval_interval=EVAL_INTERVAL
    )
    print("--- Model Training Finished ---")

    # --- Step 3: Evaluate the model ---
    print("\n--- Starting Model Evaluation (All Data) ---")
    evaluate_model(data_dir=DATA_DIR, data_split='all')
    print("--- Model Evaluation (All Data) Finished ---")

    print("\n--- Starting Model Evaluation (Validation Data) ---")
    evaluate_model(data_dir=DATA_DIR, data_split='validation')
    print("--- Model Evaluation (Validation Data) Finished ---")