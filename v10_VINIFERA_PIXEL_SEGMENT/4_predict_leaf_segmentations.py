# predict_leaf_segmentations.py

import os
import numpy as np
from PIL import Image, ImageDraw
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import json
from skimage import color, filters # Ensure scikit-image is installed: pip install scikit-image
import re

# --- CONFIGURATION (UPDATE THESE PATHS) ---
# Path to your best saved model checkpoint
MODEL_PATH = "VINIFERA2_best_model_vein_dice_0.6564_epoch18.pt" # <--- UPDATE THIS TO YOUR ACTUAL BEST MODEL PATH!

# Directory containing the images you want to predict on
# Assuming these are raw JPGs or PNGs, like your 'additional_data' format
INFERENCE_INPUT_DIR = "inference_images" # <--- CREATE THIS FOLDER AND PUT YOUR IMAGES HERE!

# Output directories
INFERENCE_OUTPUT_MASKS_DIR = "inference_output/masks"
INFERENCE_OUTPUT_OVERLAYS_DIR = "inference_output/overlays"

# Path to the preprocessing config JSON (generated by your preprocessing script)
PREPROCESSING_CONFIG_PATH = "PROCESSED_DATA_FOR_SEGMENTATION_VINIFERA/config/preprocessing_config.json"

# Create output directories if they don't exist
os.makedirs(INFERENCE_OUTPUT_MASKS_DIR, exist_ok=True)
os.makedirs(INFERENCE_OUTPUT_OVERLAYS_DIR, exist_ok=True)

# --- Overlay Colors (RGBA - Red, Green, Blue, Alpha) ---
# Background (0) is transparent, so it won't be drawn directly
BLADE_COLOR = (255, 165, 0, int(0.7 * 255)) # Orange, 70% opacity
VEIN_COLOR = (255, 0, 255, int(0.7 * 255))  # Magenta, 70% opacity

# === LOAD PREPROCESSING CONFIGURATION ===
if not os.path.exists(PREPROCESSING_CONFIG_PATH):
    raise FileNotFoundError(f"Preprocessing configuration file not found at: {PREPROCESSING_CONFIG_PATH}\n"
                            "Please ensure your preprocessing script has been run and saved the config.")
with open(PREPROCESSING_CONFIG_PATH, 'r') as f:
    PREPROCESSING_CONFIG = json.load(f)

TARGET_WIDTH = PREPROCESSING_CONFIG["TARGET_SIZE"][0]
TARGET_HEIGHT = PREPROCESSING_CONFIG["TARGET_SIZE"][1]
TARGET_SIZE = (TARGET_WIDTH, TARGET_HEIGHT)
SATO_SIGMAS = PREPROCESSING_CONFIG["sato_sigmas"]
MEIJERING_SIGMAS = PREPROCESSING_CONFIG["meijering_sigmas"]
FRANGI_SIGMAS = PREPROCESSING_CONFIG["frangi_sigmas"]
HESSIAN_SIGMAS = PREPROCESSING_CONFIG["hessian_sigmas"]
ENHANCE_PERCENTILE = PREPROCESSING_CONFIG["ENHANCE_PERCENTILE"]

IMAGE_EXTENSIONS = [".jpg", ".jpeg", ".tif", ".tiff", ".png", ".JPG", ".JPEG", ".TIF", ".TIFF", ".PNG"]

# === HELPER FUNCTIONS (COPIED FROM YOUR PREPROCESSING SCRIPT) ===
def rotate_to_wide(image_pil):
    """Rotates an image so its width is greater than its height."""
    width, height = image_pil.size
    rotation_applied = False
    if height > width:
        image_pil = image_pil.transpose(Image.Transpose.ROTATE_270)
        rotation_applied = True
    return image_pil, rotation_applied

def rescale_and_pad_image(image_pil, target_size):
    """
    Rescales an image to fit within target_size while maintaining aspect ratio,
    then pads with white (for RGB) or black (for L) to reach target_size.
    Also returns the paste offset and scaled dimensions.
    """
    original_width, original_height = image_pil.size
    target_width, target_height = target_size

    if original_width == 0 or original_height == 0:
        return Image.new("RGB", target_size, (255, 255, 255)), (0, 0), (0, 0) # Return empty and default if invalid

    scale_w = target_width / original_width
    scale_h = target_height / original_height
    scale_factor = min(scale_w, scale_h)

    new_width = int(original_width * scale_factor)
    new_height = int(original_height * scale_factor)

    scaled_img = image_pil.resize((new_width, new_height), Image.LANCZOS)

    paste_x = (target_width - new_width) // 2
    paste_y = (target_height - new_height) // 2

    padded_img = Image.new("RGB", target_size, (255, 255, 255)) # Always use white for RGB background
    padded_img.paste(scaled_img, (paste_x, paste_y))

    return padded_img, (paste_x, paste_y), (new_width, new_height)

def enhance_contrast(arr, percentile_val):
    """Applies contrast enhancement based on percentile."""
    vmax = np.percentile(arr, percentile_val)
    if vmax == 0:
        return np.zeros_like(arr, dtype=np.float32)
    arr_clipped = np.clip(arr, 0, vmax)
    arr_rescaled = arr_clipped / vmax
    return arr_rescaled.astype(np.float32)

def apply_ridge_filters(image_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p):
    """
    Applies various ridge filters to a grayscale image and returns their enhanced outputs.
    Takes a PIL Image that is already padded to TARGET_SIZE.
    """
    image_rgb_float = np.array(image_pil_padded).astype(np.float32) / 255.0

    if image_rgb_float.ndim == 3 and image_rgb_float.shape[2] == 3:
        gray_image = color.rgb2gray(image_rgb_float)
    elif image_rgb_float.ndim == 2:
        gray_image = image_rgb_float
    else:
        raise ValueError(f"Unexpected image dimensions or mode for filter application: {image_rgb_float.shape}")

    sato_raw = filters.sato(gray_image, sigmas=sato_s, black_ridges=False, mode='reflect')
    meijering_raw = filters.meijering(gray_image, sigmas=meijering_s, black_ridges=False, mode='reflect')
    frangi_raw = filters.frangi(gray_image, sigmas=frangi_s, black_ridges=False, mode='reflect')
    hessian_raw = filters.hessian(gray_image, sigmas=hessian_s, black_ridges=True, mode='reflect')

    sato_processed = enhance_contrast(sato_raw, enhance_p)
    meijering_processed = enhance_contrast(meijering_raw, enhance_p)
    frangi_processed = enhance_contrast(frangi_raw, enhance_p)
    hessian_processed = enhance_contrast(hessian_raw, enhance_p)

    return sato_processed, meijering_processed, frangi_processed, hessian_processed

def create_7channel_input(img_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p):
    """
    Creates the 7-channel input array for the UNet model.
    Channels: L, A, B, Sato, Meijering, Frangi, Hessian.
    """
    img_rgb_float = np.array(img_pil_padded).astype(np.float32) / 255.0
    img_lab = color.rgb2lab(img_rgb_float)

    L_channel = img_lab[:, :, 0] / 100.0
    A_channel = (img_lab[:, :, 1] + 128) / 255.0
    B_channel = (img_lab[:, :, 2] + 128) / 255.0

    sato_ch, meijering_ch, frangi_ch, hessian_ch = \
        apply_ridge_filters(img_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p)

    seven_channels = np.stack([L_channel, A_channel, B_channel,
                               sato_ch, meijering_ch, frangi_ch, hessian_ch], axis=-1)

    return seven_channels

# ===================== U-Net Model Definition (COPIED EXACTLY from your training script) =====================
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        # Helper function for Conv-BatchNorm-ReLU block
        def CBR(in_ch, out_ch):
            return nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )

        # Encoder path
        self.enc1 = nn.Sequential(CBR(in_channels, 64), CBR(64, 64))
        self.enc2 = nn.Sequential(CBR(64, 128), CBR(128, 128))
        self.enc3 = nn.Sequential(CBR(128, 256), CBR(256, 256))
        self.enc4 = nn.Sequential(CBR(256, 512), CBR(512, 512)) # Added another encoder block

        self.pool = nn.MaxPool2d(2)

        # Decoder path
        # Using ConvTranspose2d (upsampling)
        self.up4 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec4 = nn.Sequential(CBR(512, 256), CBR(256, 256)) # Input channels are 256 (from up4) + 256 (from enc3) = 512

        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec3 = nn.Sequential(CBR(256, 128), CBR(128, 128)) # Input channels are 128 (from up3) + 128 (from enc2) = 256

        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec2 = nn.Sequential(CBR(128, 64), CBR(64, 64)) # Input channels are 64 (from up2) + 64 (from enc1) = 128

        self.out_conv = nn.Conv2d(64, out_channels, 1)

    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3)) # New encoder block

        # Decoder
        d4 = self.up4(e4)
        # Handle potential size mismatch for skip connections (due to odd dimensions or padding)
        if d4.shape != e3.shape:
             d4 = F.interpolate(d4, size=e3.shape[2:], mode='bilinear', align_corners=False)
        d4 = torch.cat([d4, e3], dim=1) # Concatenate skip connection
        d4 = self.dec4(d4)

        d3 = self.up3(d4)
        if d3.shape != e2.shape:
            d3 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)
        d3 = torch.cat([d3, e2], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up2(d3)
        if d2.shape != e1.shape:
            d2 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)
        d2 = torch.cat([d2, e1], dim=1)
        d2 = self.dec2(d2)

        return self.out_conv(d2)

# === MAIN PREDICTION LOGIC ===
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load model
    model = UNet(in_channels=7, out_channels=3) # 7 input channels, 3 output classes (0,1,2)

    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"Model checkpoint not found at: {MODEL_PATH}\n"
                                "Please update MODEL_PATH to your best saved model.")

    # Load the state_dict with the correct map_location
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval() # Set model to evaluation mode

    print(f"Loaded model from: {MODEL_PATH}")
    print(f"Reading images from: {INFERENCE_INPUT_DIR}")
    print(f"Saving masks to: {INFERENCE_OUTPUT_MASKS_DIR}")
    print(f"Saving overlays to: {INFERENCE_OUTPUT_OVERLAYS_DIR}")

    # Ensure we only pick up actual image files and handle case sensitivity for extensions
    image_filenames = [f for f in os.listdir(INFERENCE_INPUT_DIR) if any(f.lower().endswith(ext.lower()) for ext in IMAGE_EXTENSIONS)]

    if not image_filenames:
        print(f"No image files found in '{INFERENCE_INPUT_DIR}'. Exiting.")
        return

    print(f"Found {len(image_filenames)} images for prediction.")

    # Helper function for drawing masks on overlays (improved for clarity)
    def draw_mask_on_overlay(overlay_img, mask_arr, color, class_val):
        draw = ImageDraw.Draw(overlay_img)
        # Get coordinates where the mask matches the current class value
        y_coords, x_coords = np.where(mask_arr == class_val)

        # Draw a point for each pixel. For very large images, this can still be the primary bottleneck.
        # However, it's generally robust and straightforward.
        for y, x in zip(y_coords, x_coords):
            draw.point((x, y), fill=color)
        return overlay_img

    for img_filename in tqdm(image_filenames, desc="Predicting on images"):
        fid = os.path.splitext(img_filename)[0] # File ID without extension

        # Construct the full path to the input image, checking for various extensions
        input_image_full_path = None
        for ext in IMAGE_EXTENSIONS:
            if os.path.exists(os.path.join(INFERENCE_INPUT_DIR, fid + ext)):
                input_image_full_path = os.path.join(INFERENCE_INPUT_DIR, fid + ext)
                break
            # Also check lowercase extensions
            elif os.path.exists(os.path.join(INFERENCE_INPUT_DIR, fid + ext.lower())):
                input_image_full_path = os.path.join(INFERENCE_INPUT_DIR, fid + ext.lower())
                break
        
        if input_image_full_path is None:
            print(f"WARNING: Could not find image file for FID '{fid}' with any expected extension. Skipping.")
            continue


        output_mask_path = os.path.join(INFERENCE_OUTPUT_MASKS_DIR, f"{fid}_mask.png")
        output_overlay_path = os.path.join(INFERENCE_OUTPUT_OVERLAYS_DIR, f"{fid}_overlay.png")

        # Skip if outputs already exist (useful for resuming a run)
        if os.path.exists(output_mask_path) and os.path.exists(output_overlay_path):
            # print(f"Skipping {fid}: Outputs already exist.") # Uncomment for verbose skipping
            continue

        try:
            # 1. Load original image
            original_pil_img = Image.open(input_image_full_path).convert("RGB")

            # 2. Preprocess for 7-channels (matching training)
            # This 'img_pil_padded_for_overlay' will be the base for our overlay.
            # It's the original image, after rotation and padding to the TARGET_SIZE.
            img_pil_preprocessed_rot, _ = rotate_to_wide(original_pil_img.copy())
            img_pil_padded_for_overlay, _, _ = rescale_and_pad_image(img_pil_preprocessed_rot, TARGET_SIZE)

            # Now, generate the 7-channel input from this padded image for the model
            seven_channel_data_np = create_7channel_input(
                img_pil_padded_for_overlay, SATO_SIGMAS, MEIJERING_SIGMAS, FRANGI_SIGMAS, HESSIAN_SIGMAS, ENHANCE_PERCENTILE
            )

            # Convert numpy array to PyTorch tensor, add batch dimension, move to device
            input_tensor = torch.from_numpy(seven_channel_data_np).permute(2, 0, 1).float().unsqueeze(0).to(device)

            # 3. Perform prediction
            with torch.no_grad(): # Disable gradient calculation for inference (saves memory and speeds up)
                outputs = model(input_tensor)
                # Apply softmax to get probabilities, then argmax to get the class with highest probability for each pixel
                predicted_mask = torch.argmax(F.softmax(outputs, dim=1), dim=1).squeeze(0).cpu().numpy()

            # 4. Save 0,1,2 mask
            # Convert the NumPy array mask to a PIL Image (mode 'L' for grayscale/palette)
            mask_img = Image.fromarray(predicted_mask.astype(np.uint8), mode='L')
            mask_img.save(output_mask_path)

            # 5. Create and save RGB overlay
            # Convert the padded base image to RGBA (for alpha channel)
            base_image_rgba = img_pil_padded_for_overlay.convert("RGBA")
            # Create a new, fully transparent overlay image
            overlay = Image.new('RGBA', TARGET_SIZE, (0, 0, 0, 0))

            # Apply blade overlay (Class 1)
            overlay = draw_mask_on_overlay(overlay, predicted_mask, BLADE_COLOR, 1)
            
            # Apply vein overlay (Class 2) - veins will draw on top of blade if they overlap
            overlay = draw_mask_on_overlay(overlay, predicted_mask, VEIN_COLOR, 2)
            
            # Composite (blend) the overlay onto the base image
            final_overlay_img = Image.alpha_composite(base_image_rgba, overlay)
            final_overlay_img.save(output_overlay_path)

        except Exception as e:
            print(f"‚ùå Error processing {img_filename}: {e}. Skipping this image.")

    print("\n--- Prediction Complete ---")
    print(f"Masks saved to: {INFERENCE_OUTPUT_MASKS_DIR}")
    print(f"Overlays saved to: {INFERENCE_OUTPUT_OVERLAYS_DIR}")

if __name__ == '__main__':
    main()