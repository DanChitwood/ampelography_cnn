# predict_leaf_segmentations.py

import os
import numpy as np
from PIL import Image, ImageDraw
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import json
from skimage import color, filters # Ensure scikit-image is installed: pip install scikit-image
import re

# --- CONFIGURATION (UPDATE THESE PATHS) ---
# Path to your best saved model checkpoint
MODEL_PATH = "V1_VINI_best_model_vein_dice_0.6547_epoch43.pt" # <--- UPDATE THIS TO YOUR ACTUAL BEST MODEL PATH!

# Directory containing the images you want to predict on
# Assuming these are raw JPGs or PNGs, like your 'additional_data' format
INFERENCE_INPUT_DIR = "inference_images" # <--- CREATE THIS FOLDER AND PUT YOUR IMAGES HERE!

# Output directories
INFERENCE_OUTPUT_MASKS_DIR = "inference_output/masks"
INFERENCE_OUTPUT_OVERLAYS_DIR = "inference_output/overlays"

# Path to the preprocessing config JSON (generated by your preprocessing script)
PREPROCESSING_CONFIG_PATH = "PROCESSED_DATA_FOR_SEGMENTATION_VINIFERA/config/preprocessing_config.json"

# Create output directories if they don't exist
os.makedirs(INFERENCE_OUTPUT_MASKS_DIR, exist_ok=True)
os.makedirs(INFERENCE_OUTPUT_OVERLAYS_DIR, exist_ok=True)

# --- Overlay Colors (RGBA - Red, Green, Blue, Alpha) ---
# Background (0) is transparent, so it won't be drawn directly
BLADE_COLOR = (255, 165, 0, int(0.7 * 255)) # Orange, 70% opacity
VEIN_COLOR = (255, 0, 255, int(0.7 * 255)) # Magenta, 70% opacity

# === LOAD PREPROCESSING CONFIGURATION ===
if not os.path.exists(PREPROCESSING_CONFIG_PATH):
    raise FileNotFoundError(f"Preprocessing configuration file not found at: {PREPROCESSING_CONFIG_PATH}\n"
                            "Please ensure your preprocessing script has been run and saved the config.")
with open(PREPROCESSING_CONFIG_PATH, 'r') as f:
    PREPROCESSING_CONFIG = json.load(f)

TARGET_WIDTH = PREPROCESSING_CONFIG["TARGET_SIZE"][0]
TARGET_HEIGHT = PREPROCESSING_CONFIG["TARGET_SIZE"][1]
TARGET_SIZE = (TARGET_WIDTH, TARGET_HEIGHT)
SATO_SIGMAS = PREPROCESSING_CONFIG["sato_sigmas"]
MEIJERING_SIGMAS = PREPROCESSING_CONFIG["meijering_sigmas"]
FRANGI_SIGMAS = PREPROCESSING_CONFIG["frangi_sigmas"]
HESSIAN_SIGMAS = PREPROCESSING_CONFIG["hessian_sigmas"]
ENHANCE_PERCENTILE = PREPROCESSING_CONFIG["ENHANCE_PERCENTILE"]

IMAGE_EXTENSIONS = [".jpg", ".jpeg", ".tif", ".tiff", ".png", ".JPG", ".JPEG", ".TIF", ".TIFF", ".PNG"]

# === HELPER FUNCTIONS (COPIED AND MODIFIED FROM YOUR PREPROCESSING SCRIPT) ===
def rotate_to_wide(image_pil):
    """Rotates an image so its width is greater than its height."""
    width, height = image_pil.size
    rotation_applied = False
    if height > width:
        image_pil = image_pil.transpose(Image.Transpose.ROTATE_270)
        rotation_applied = True
    return image_pil, rotation_applied

def rescale_and_pad_image(image_pil, target_size):
    """
    Rescales an image to fit within target_size while maintaining aspect ratio,
    then pads with white (for RGB) or black (for L) to reach target_size.
    Also returns the paste offset, scaled dimensions, and scale factor.
    """
    original_width, original_height = image_pil.size
    target_width, target_height = target_size
    
    scale_w = target_width / original_width
    scale_h = target_height / original_height
    scale_factor = min(scale_w, scale_h)

    if original_width == 0 or original_height == 0:
        return Image.new("RGB", target_size, (255, 255, 255)), (0, 0), (0, 0), 0.0 # Return empty and default if invalid

    new_width = int(original_width * scale_factor)
    new_height = int(original_height * scale_factor)

    scaled_img = image_pil.resize((new_width, new_height), Image.LANCZOS)

    paste_x = (target_width - new_width) // 2
    paste_y = (target_height - new_height) // 2

    # Use white for RGB background. In preprocessing script, it's white for RGB, black for L. Here, assuming RGB.
    padded_img = Image.new("RGB", target_size, (255, 255, 255))
    padded_img.paste(scaled_img, (paste_x, paste_y))
    
    return padded_img, (paste_x, paste_y), (new_width, new_height), scale_factor


def enhance_contrast(arr, percentile_val):
    """Applies contrast enhancement based on percentile."""
    vmax = np.percentile(arr, percentile_val)
    if vmax == 0:
        return np.zeros_like(arr, dtype=np.float32)
    arr_clipped = np.clip(arr, 0, vmax)
    arr_rescaled = arr_clipped / vmax
    return arr_rescaled.astype(np.float32)

def apply_ridge_filters(image_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p):
    """
    Applies various ridge filters to a grayscale image and returns their enhanced outputs.
    Generates both black_ridges=True and False versions for Sato, Meijering, Frangi.
    Generates both black_ridges=True and False versions for Hessian.
    Takes a PIL Image that is already padded to TARGET_SIZE.
    """
    image_rgb_float = np.array(image_pil_padded).astype(np.float32) / 255.0
    
    if image_rgb_float.ndim == 3 and image_rgb_float.shape[2] == 3:
        gray_image = color.rgb2gray(image_rgb_float)
    elif image_rgb_float.ndim == 2:
        gray_image = image_rgb_float
    else:
        raise ValueError(f"Unexpected image dimensions or mode for filter application: {image_rgb_float.shape}")

    # --- Sato Filter ---
    sato_br_false_raw = filters.sato(gray_image, sigmas=sato_s, black_ridges=False, mode='reflect')
    sato_br_true_raw = filters.sato(gray_image, sigmas=sato_s, black_ridges=True, mode='reflect')
    sato_br_false_processed = enhance_contrast(sato_br_false_raw, enhance_p)
    sato_br_true_processed = enhance_contrast(sato_br_true_raw, enhance_p)

    # --- Meijering Filter ---
    meijering_br_false_raw = filters.meijering(gray_image, sigmas=meijering_s, black_ridges=False, mode='reflect')
    meijering_br_true_raw = filters.meijering(gray_image, sigmas=meijering_s, black_ridges=True, mode='reflect')
    meijering_br_false_processed = enhance_contrast(meijering_br_false_raw, enhance_p)
    meijering_br_true_processed = enhance_contrast(meijering_br_true_raw, enhance_p)

    # --- Frangi Filter ---
    frangi_br_false_raw = filters.frangi(gray_image, sigmas=frangi_s, black_ridges=False, mode='reflect')
    frangi_br_true_raw = filters.frangi(gray_image, sigmas=frangi_s, black_ridges=True, mode='reflect')
    frangi_br_false_processed = enhance_contrast(frangi_br_false_raw, enhance_p)
    frangi_br_true_processed = enhance_contrast(frangi_br_true_raw, enhance_p)

    # --- Hessian Filter ---
    hessian_br_true_raw = filters.hessian(gray_image, sigmas=hessian_s, black_ridges=True, mode='reflect')
    hessian_br_false_raw = filters.hessian(gray_image, sigmas=hessian_s, black_ridges=False, mode='reflect')
    hessian_br_true_processed = enhance_contrast(hessian_br_true_raw, enhance_p)
    hessian_br_false_processed = enhance_contrast(hessian_br_false_raw, enhance_p)

    return (sato_br_false_processed, sato_br_true_processed,
            meijering_br_false_processed, meijering_br_true_processed,
            frangi_br_false_processed, frangi_br_true_processed,
            hessian_br_true_processed, hessian_br_false_processed) # 8 channels

def create_11channel_input(img_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p):
    """
    Creates the 11-channel input array for the UNet model, matching the preprocessing script.
    Channels: L, A, B, Sato_F, Sato_T, Meijering_F, Meijering_T, Frangi_F, Frangi_T, Hessian_T, Hessian_F.
    """
    img_rgb_float = np.array(img_pil_padded).astype(np.float32) / 255.0
    img_lab = color.rgb2lab(img_rgb_float)

    L_channel = img_lab[:, :, 0] / 100.0  # L channel normalized to 0-1
    A_channel = (img_lab[:, :, 1] + 128) / 255.0  # A channel normalized to 0-1
    B_channel = (img_lab[:, :, 2] + 128) / 255.0  # B channel normalized to 0-1

    sato_f, sato_t, meijering_f, meijering_t, frangi_f, frangi_t, hessian_t, hessian_f = \
        apply_ridge_filters(img_pil_padded, sato_s, meijering_s, frangi_s, hessian_s, enhance_p)

    eleven_channels = np.stack([L_channel, A_channel, B_channel,
                                 sato_f, sato_t, meijering_f, meijering_t,
                                 frangi_f, frangi_t, hessian_t, hessian_f], axis=-1)
    
    return eleven_channels


# ===================== U-Net Model Definition (CORRECTED to match your saved model's output heads) =====================
class UNet(nn.Module):
    # Modified: Separate output channels for segmentation and geodesic tasks
    def __init__(self, in_channels, num_seg_classes, num_geo_channels):
        super().__init__()

        # Helper function for Conv-BatchNorm-ReLU block
        def CBR(in_ch, out_ch):
            return nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )

        # Encoder path
        self.enc1 = nn.Sequential(CBR(in_channels, 64), CBR(64, 64))
        self.enc2 = nn.Sequential(CBR(64, 128), CBR(128, 128))
        self.enc3 = nn.Sequential(CBR(128, 256), CBR(256, 256))
        self.enc4 = nn.Sequential(CBR(256, 512), CBR(512, 512))

        self.pool = nn.MaxPool2d(2)

        # Decoder path
        self.up4 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec4 = nn.Sequential(CBR(512, 256), CBR(256, 256))

        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec3 = nn.Sequential(CBR(256, 128), CBR(128, 128))

        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec2 = nn.Sequential(CBR(128, 64), CBR(64, 64))

        # MODIFIED: Separate output heads as implied by your saved model's state_dict keys
        self.seg_out_conv = nn.Conv2d(64, num_seg_classes, 1) # Output for segmentation (3 classes: background, blade, vein)
        # Assuming geo_out_conv also had a sigmoid if its target was normalized (e.g., 0-1)
        # The key was "geo_out_conv.0.weight" suggesting it was part of a Sequential.
        self.geo_out_conv = nn.Sequential(
            nn.Conv2d(64, num_geo_channels, 1),
            nn.Sigmoid() # Common for normalized regression outputs
        )

    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        # Decoder
        d4 = self.up4(e4)
        if d4.shape != e3.shape:
            # Added for robustness, though usually upsample handles size mismatches if strides are correct
            d4 = F.interpolate(d4, size=e3.shape[2:], mode='bilinear', align_corners=False)
        d4 = torch.cat([d4, e3], dim=1)
        d4 = self.dec4(d4)

        d3 = self.up3(d4)
        if d3.shape != e2.shape:
            d3 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)
        d3 = torch.cat([d3, e2], dim=1)
        d3 = self.dec3(d3)

        d2 = self.up2(d3)
        if d2.shape != e1.shape:
            d2 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)
        d2 = torch.cat([d2, e1], dim=1)
        d2 = self.dec2(d2)

        # MODIFIED: Pass through separate output heads
        seg_output = self.seg_out_conv(d2)
        geo_output = self.geo_out_conv(d2)
        
        return seg_output, geo_output


# === MAIN PREDICTION LOGIC ===
def main():
    # MODIFIED: Prioritize MPS for Apple Silicon
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print("Using device: mps (Apple Silicon GPU)")
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print("Using device: cuda (NVIDIA GPU)")
    else:
        device = torch.device('cpu')
        print("Using device: cpu")

    # Load model
    # MODIFIED: Pass correct output channel arguments to UNet constructor
    # 3 classes for segmentation (background, blade, vein), 1 channel for geodesic
    model = UNet(in_channels=11, num_seg_classes=3, num_geo_channels=1) 

    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"Model checkpoint not found at: {MODEL_PATH}\n"
                                "Please update MODEL_PATH to your best saved model.")

    # Load the state_dict with the correct map_location
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval() # Set model to evaluation mode

    print(f"Loaded model from: {MODEL_PATH}")
    print(f"Reading images from: {INFERENCE_INPUT_DIR}")
    print(f"Saving masks to: {INFERENCE_OUTPUT_MASKS_DIR}")
    print(f"Saving overlays to: {INFERENCE_OUTPUT_OVERLAYS_DIR}")

    # Ensure we only pick up actual image files and handle case sensitivity for extensions
    image_filenames = [f for f in os.listdir(INFERENCE_INPUT_DIR) if any(f.lower().endswith(ext.lower()) for ext in IMAGE_EXTENSIONS)]

    if not image_filenames:
        print(f"No image files found in '{INFERENCE_INPUT_DIR}'. Exiting.")
        return

    print(f"Found {len(image_filenames)} images for prediction.")

    # Helper function for drawing masks on overlays (improved for clarity)
    def draw_mask_on_overlay(overlay_img, mask_arr, color, class_val):
        draw = ImageDraw.Draw(overlay_img)
        # Get coordinates where the mask matches the current class value
        y_coords, x_coords = np.where(mask_arr == class_val)

        for y, x in zip(y_coords, x_coords):
            draw.point((x, y), fill=color)
        return overlay_img

    for img_filename in tqdm(image_filenames, desc="Predicting on images"):
        fid = os.path.splitext(img_filename)[0] # File ID without extension

        # Construct the full path to the input image, checking for various extensions
        input_image_full_path = None
        for ext in IMAGE_EXTENSIONS:
            if os.path.exists(os.path.join(INFERENCE_INPUT_DIR, fid + ext)):
                input_image_full_path = os.path.join(INFERENCE_INPUT_DIR, fid + ext)
                break
            elif os.path.exists(os.path.join(INFERENCE_INPUT_DIR, fid + ext.lower())):
                input_image_full_path = os.path.join(INFERENCE_INPUT_DIR, fid + ext.lower())
                break
            
        if input_image_full_path is None:
            print(f"WARNING: Could not find image file for FID '{fid}' with any expected extension. Skipping.")
            continue


        output_mask_path = os.path.join(INFERENCE_OUTPUT_MASKS_DIR, f"{fid}_mask.png")
        output_overlay_path = os.path.join(INFERENCE_OUTPUT_OVERLAYS_DIR, f"{fid}_overlay.png")

        # Skip if outputs already exist (useful for resuming a run)
        if os.path.exists(output_mask_path) and os.path.exists(output_overlay_path):
            # print(f"Skipping {fid}: Outputs already exist.") # Uncomment for verbose skipping
            continue

        try:
            # 1. Load original image
            original_pil_img = Image.open(input_image_full_path).convert("RGB")

            # 2. Preprocess for 11-channels (matching training)
            img_pil_preprocessed_rot, _ = rotate_to_wide(original_pil_img.copy())
            img_pil_padded_for_overlay, _, _, scale_factor_applied = rescale_and_pad_image(img_pil_preprocessed_rot, TARGET_SIZE)

            # Now, generate the 11-channel input from this padded image for the model
            eleven_channel_data_np = create_11channel_input(
                img_pil_padded_for_overlay, SATO_SIGMAS, MEIJERING_SIGMAS, FRANGI_SIGMAS, HESSIAN_SIGMAS, ENHANCE_PERCENTILE
            )

            # Convert numpy array to PyTorch tensor, add batch dimension, move to device
            input_tensor = torch.from_numpy(eleven_channel_data_np).permute(2, 0, 1).float().unsqueeze(0).to(device)

            # 3. Perform prediction
            with torch.no_grad(): # Disable gradient calculation for inference (saves memory and speeds up)
                seg_outputs, geo_outputs = model(input_tensor) 
                predicted_mask = torch.argmax(F.softmax(seg_outputs, dim=1), dim=1).squeeze(0).cpu().numpy()
                # predicted_geodesic_map (geo_outputs) is generated but not explicitly used/saved in this script

            # 4. Save 0,1,2 mask
            mask_img = Image.fromarray(predicted_mask.astype(np.uint8), mode='L')
            mask_img.save(output_mask_path)

            # 5. Create and save RGB overlay
            base_image_rgba = img_pil_padded_for_overlay.convert("RGBA")
            overlay = Image.new('RGBA', TARGET_SIZE, (0, 0, 0, 0))

            # Apply blade overlay (Class 1)
            overlay = draw_mask_on_overlay(overlay, predicted_mask, BLADE_COLOR, 1)
            
            # Apply vein overlay (Class 2) - veins will draw on top of blade if they overlap
            overlay = draw_mask_on_overlay(overlay, predicted_mask, VEIN_COLOR, 2)
            
            final_overlay_img = Image.alpha_composite(base_image_rgba, overlay)
            final_overlay_img.save(output_overlay_path)

        except Exception as e:
            print(f"‚ùå Error processing {img_filename}: {e}. Skipping this image.")

    print("\n--- Prediction Complete ---")
    print(f"Masks saved to: {INFERENCE_OUTPUT_MASKS_DIR}")
    print(f"Overlays saved to: {INFERENCE_OUTPUT_OVERLAYS_DIR}")

if __name__ == '__main__':
    main()